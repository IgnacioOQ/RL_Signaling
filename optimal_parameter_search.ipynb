{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52581437",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "356d659d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imports import *\n",
    "from utils import *\n",
    "from agents import UrnAgent, QLearningAgent, TDLearningAgent\n",
    "from environment import NetMultiAgentEnv, TempNetMultiAgentEnv\n",
    "from simulation_function import simulation_function, temp_simulation_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dada83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_results(search_results, top_n=5, sort_by='mean_reward'):\n",
    "    \"\"\"\n",
    "    Takes parameter search results and shows the top configurations and summaries.\n",
    "    \"\"\"\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame([\n",
    "        {**res['params'], 'mean_reward': res['mean_reward'], 'std_reward': res['std_reward'], 'mean_final_nmi': res.get('mean_final_nmi', None)}\n",
    "        for res in search_results\n",
    "    ])\n",
    "\n",
    "    # Sort and show top-N\n",
    "    top_df = df.sort_values(by=sort_by, ascending=False).head(top_n)\n",
    "    print(f\"🔝 Top {top_n} Configurations by {sort_by}:\\n\")\n",
    "    print(top_df.to_string(index=False))\n",
    "\n",
    "    return df, top_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ffc5d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_param_sensitivity(df, reward_col='mean_reward', error_col='std_reward', nmi_col='mean_final_nmi'):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    param_cols = [col for col in df.columns if col not in [reward_col, error_col, nmi_col]]\n",
    "\n",
    "    num_params = len(param_cols)\n",
    "    fig, axes = plt.subplots(num_params, 1, figsize=(8, 4 * num_params))\n",
    "\n",
    "    if num_params == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for i, param in enumerate(param_cols):\n",
    "        ax = axes[i]\n",
    "        grouped = df.groupby(param).agg({reward_col: 'mean', error_col: 'mean', nmi_col: 'mean'}).reset_index()\n",
    "\n",
    "        ax.errorbar(grouped[param], grouped[reward_col], yerr=grouped[error_col], fmt='o-', capsize=5, label='Mean Reward')\n",
    "        ax.set_xlabel(param)\n",
    "        ax.set_ylabel('Mean Reward')\n",
    "        ax.set_title(f'Effect of {param} on Reward')\n",
    "\n",
    "        if nmi_col in df.columns:\n",
    "            for j, row in grouped.iterrows():\n",
    "                ax.annotate(f\"NMI={row[nmi_col]:.2f}\", (row[param], row[reward_col]), fontsize=8)\n",
    "\n",
    "        ax.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dfe2b2",
   "metadata": {},
   "source": [
    "# QLearning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80eeec9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tested: {'exploration_rate': 1.0, 'exploration_decay': 0.999, 'min_exploration_rate': 0.05} => Mean Final Reward: 0.715, Mean Final NMI: 0.238, Std Reward: 0.170\n",
      "Tested: {'exploration_rate': 1.0, 'exploration_decay': 0.999, 'min_exploration_rate': 0.0001} => Mean Final Reward: 0.738, Mean Final NMI: 0.239, Std Reward: 0.146\n",
      "Tested: {'exploration_rate': 1.0, 'exploration_decay': 0.995, 'min_exploration_rate': 0.05} => Mean Final Reward: 0.719, Mean Final NMI: 0.377, Std Reward: 0.151\n",
      "Tested: {'exploration_rate': 1.0, 'exploration_decay': 0.995, 'min_exploration_rate': 0.0001} => Mean Final Reward: 0.767, Mean Final NMI: 0.441, Std Reward: 0.163\n",
      "Tested: {'exploration_rate': 1.0, 'exploration_decay': 0.99, 'min_exploration_rate': 0.05} => Mean Final Reward: 0.695, Mean Final NMI: 0.358, Std Reward: 0.166\n",
      "Tested: {'exploration_rate': 1.0, 'exploration_decay': 0.99, 'min_exploration_rate': 0.0001} => Mean Final Reward: 0.741, Mean Final NMI: 0.449, Std Reward: 0.174\n",
      "Tested: {'exploration_rate': 1.0, 'exploration_decay': 0.9, 'min_exploration_rate': 0.05} => Mean Final Reward: 0.704, Mean Final NMI: 0.342, Std Reward: 0.159\n",
      "Tested: {'exploration_rate': 1.0, 'exploration_decay': 0.9, 'min_exploration_rate': 0.0001} => Mean Final Reward: 0.475, Mean Final NMI: 0.449, Std Reward: 0.142\n",
      "Tested: {'exploration_rate': 0.75, 'exploration_decay': 0.999, 'min_exploration_rate': 0.05} => Mean Final Reward: 0.727, Mean Final NMI: 0.278, Std Reward: 0.163\n",
      "Tested: {'exploration_rate': 0.75, 'exploration_decay': 0.999, 'min_exploration_rate': 0.0001} => Mean Final Reward: 0.711, Mean Final NMI: 0.240, Std Reward: 0.169\n",
      "Tested: {'exploration_rate': 0.75, 'exploration_decay': 0.995, 'min_exploration_rate': 0.05} => Mean Final Reward: 0.694, Mean Final NMI: 0.335, Std Reward: 0.168\n",
      "Tested: {'exploration_rate': 0.75, 'exploration_decay': 0.995, 'min_exploration_rate': 0.0001} => Mean Final Reward: 0.745, Mean Final NMI: 0.425, Std Reward: 0.182\n",
      "Tested: {'exploration_rate': 0.75, 'exploration_decay': 0.99, 'min_exploration_rate': 0.05} => Mean Final Reward: 0.701, Mean Final NMI: 0.365, Std Reward: 0.160\n",
      "Tested: {'exploration_rate': 0.75, 'exploration_decay': 0.99, 'min_exploration_rate': 0.0001} => Mean Final Reward: 0.720, Mean Final NMI: 0.425, Std Reward: 0.162\n",
      "Tested: {'exploration_rate': 0.75, 'exploration_decay': 0.9, 'min_exploration_rate': 0.05} => Mean Final Reward: 0.693, Mean Final NMI: 0.308, Std Reward: 0.167\n",
      "Tested: {'exploration_rate': 0.75, 'exploration_decay': 0.9, 'min_exploration_rate': 0.0001} => Mean Final Reward: 0.424, Mean Final NMI: 0.414, Std Reward: 0.131\n",
      "Tested: {'exploration_rate': 0.5, 'exploration_decay': 0.999, 'min_exploration_rate': 0.05} => Mean Final Reward: 0.728, Mean Final NMI: 0.326, Std Reward: 0.174\n",
      "Tested: {'exploration_rate': 0.5, 'exploration_decay': 0.999, 'min_exploration_rate': 0.0001} => Mean Final Reward: 0.731, Mean Final NMI: 0.290, Std Reward: 0.181\n",
      "Tested: {'exploration_rate': 0.5, 'exploration_decay': 0.995, 'min_exploration_rate': 0.05} => Mean Final Reward: 0.708, Mean Final NMI: 0.359, Std Reward: 0.152\n",
      "Tested: {'exploration_rate': 0.5, 'exploration_decay': 0.995, 'min_exploration_rate': 0.0001} => Mean Final Reward: 0.770, Mean Final NMI: 0.480, Std Reward: 0.175\n",
      "Tested: {'exploration_rate': 0.5, 'exploration_decay': 0.99, 'min_exploration_rate': 0.05} => Mean Final Reward: 0.690, Mean Final NMI: 0.351, Std Reward: 0.166\n",
      "Tested: {'exploration_rate': 0.5, 'exploration_decay': 0.99, 'min_exploration_rate': 0.0001} => Mean Final Reward: 0.684, Mean Final NMI: 0.420, Std Reward: 0.161\n",
      "Tested: {'exploration_rate': 0.5, 'exploration_decay': 0.9, 'min_exploration_rate': 0.05} => Mean Final Reward: 0.703, Mean Final NMI: 0.311, Std Reward: 0.165\n",
      "Tested: {'exploration_rate': 0.5, 'exploration_decay': 0.9, 'min_exploration_rate': 0.0001} => Mean Final Reward: 0.384, Mean Final NMI: 0.301, Std Reward: 0.112\n",
      "Tested: {'exploration_rate': 0.25, 'exploration_decay': 0.999, 'min_exploration_rate': 0.05} => Mean Final Reward: 0.723, Mean Final NMI: 0.357, Std Reward: 0.148\n"
     ]
    }
   ],
   "source": [
    "def parameter_search(\n",
    "    param_grid,\n",
    "    n_trials=3,\n",
    "    n_episodes=10000,\n",
    "    base_seed=42\n",
    "):\n",
    "    results = []\n",
    "\n",
    "    # Generate all combinations of hyperparameters\n",
    "    keys = list(param_grid.keys())\n",
    "    combos = list(itertools.product(*[param_grid[k] for k in keys]))\n",
    "\n",
    "    for combo in combos:\n",
    "        params = dict(zip(keys, combo))\n",
    "        avg_rewards = []\n",
    "        avg_final_nmi = []\n",
    "\n",
    "        for trial in range(n_trials):\n",
    "            seed = base_seed + trial\n",
    "            np.random.seed(seed)\n",
    "\n",
    "            # Setup graph\n",
    "            G = nx.DiGraph()\n",
    "            G.add_nodes_from([0, 1])\n",
    "            G.add_edges_from([(0, 1), (1, 0)])\n",
    "\n",
    "            # Setup game and env\n",
    "            n_agents = 2\n",
    "            n_features = 2\n",
    "            n_signaling_actions = 2\n",
    "            n_final_actions = 4\n",
    "            agents_observed_variables = {0: [0], 1: [1]}\n",
    "            game = {i: create_random_canonical_game(n_features, n_final_actions, n=1, m=0)\n",
    "                    for i in range(n_agents)}\n",
    "\n",
    "            env = NetMultiAgentEnv(\n",
    "                n_agents=n_agents,\n",
    "                n_features=n_features,\n",
    "                n_signaling_actions=n_signaling_actions,\n",
    "                n_final_actions=n_final_actions,\n",
    "                full_information=False,\n",
    "                game_dicts=game,\n",
    "                observed_variables=agents_observed_variables,\n",
    "                agent_type=QLearningAgent,\n",
    "                initialize=False,\n",
    "                graph=G\n",
    "            )\n",
    "\n",
    "            # Override agents manually with hyperparameters\n",
    "            env.agents = [\n",
    "                QLearningAgent(\n",
    "                    n_signaling_actions=n_signaling_actions,\n",
    "                    n_final_actions=n_final_actions,\n",
    "                    exploration_rate=params['exploration_rate'],\n",
    "                    exploration_decay=params['exploration_decay'],\n",
    "                    min_exploration_rate=params['min_exploration_rate'],\n",
    "                    initialize=False\n",
    "                ) for _ in range(n_agents)\n",
    "            ]\n",
    "\n",
    "            _, rewards_history, signal_information_history, _, _ = simulation_function(\n",
    "                n_agents=n_agents,\n",
    "                n_features=n_features,\n",
    "                n_signaling_actions=n_signaling_actions,\n",
    "                n_final_actions=n_final_actions,\n",
    "                n_episodes=n_episodes,\n",
    "                with_signals=True,\n",
    "                plot=False,\n",
    "                env=env,\n",
    "                verbose=False\n",
    "            )\n",
    "\n",
    "            # Measure average reward in last 10% of episodes\n",
    "            final_rewards = [\n",
    "                np.mean(rewards[-n_episodes // 10:]) for rewards in rewards_history\n",
    "            ]\n",
    "            avg_rewards.append(np.mean(final_rewards))\n",
    "\n",
    "            # Measure final normalized mutual information\n",
    "            final_nmi = [\n",
    "                np.mean(agent_nmi[-n_episodes // 10:]) if len(agent_nmi) >= n_episodes // 10 else 0.0\n",
    "                for agent_nmi in signal_information_history\n",
    "            ]\n",
    "            avg_final_nmi.append(np.mean(final_nmi))\n",
    "\n",
    "        result = {\n",
    "            'params': params,\n",
    "            'mean_reward': np.mean(avg_rewards),\n",
    "            'std_reward': np.std(avg_rewards),\n",
    "            'mean_final_nmi': np.mean(avg_final_nmi)\n",
    "        }\n",
    "        results.append(result)\n",
    "        print(f\"Tested: {params} => Mean Final Reward: {result['mean_reward']:.3f}, Mean Final NMI: {result['mean_final_nmi']:.3f}, Std Reward: {result['std_reward']:.3f}\")\n",
    "\n",
    "    return sorted(results, key=lambda r: -r['mean_reward'])\n",
    "\n",
    "\n",
    "# Example usage\n",
    "param_grid = {\n",
    "    'exploration_rate': [1.0, 0.75, 0.5,0.25],\n",
    "    'exploration_decay': [0.999,0.995, 0.99, 0.9],\n",
    "    'min_exploration_rate': [0.1,0.05,0.001,0.0001]\n",
    "}\n",
    "\n",
    "search_results = parameter_search(param_grid, n_trials=100, n_episodes=5000)\n",
    "\n",
    "for r in search_results:\n",
    "    print(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e86fbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df, top_configs = analyze_results(search_results, top_n=5)\n",
    "top_k_df = full_df.sort_values(by='mean_reward', ascending=False).head(10)\n",
    "plot_param_sensitivity(top_k_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90afe00c",
   "metadata": {},
   "source": [
    "# TD Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80987ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_search(\n",
    "    param_grid,\n",
    "    n_trials=3,\n",
    "    n_episodes=10000,\n",
    "    base_seed=42\n",
    "):\n",
    "    results = []\n",
    "\n",
    "    # Generate all combinations of hyperparameters\n",
    "    keys = list(param_grid.keys())\n",
    "    combos = list(itertools.product(*[param_grid[k] for k in keys]))\n",
    "\n",
    "    for combo in combos:\n",
    "        params = dict(zip(keys, combo))\n",
    "        avg_rewards = []\n",
    "        avg_final_nmi = []\n",
    "\n",
    "        for trial in range(n_trials):\n",
    "            seed = base_seed + trial\n",
    "            np.random.seed(seed)\n",
    "\n",
    "            # Setup graph\n",
    "            G = nx.DiGraph()\n",
    "            G.add_nodes_from([0, 1])\n",
    "            G.add_edges_from([(0, 1), (1, 0)])\n",
    "\n",
    "            # Setup game and env\n",
    "            n_agents = 2\n",
    "            n_features = 2\n",
    "            n_signaling_actions = 2\n",
    "            n_final_actions = 4\n",
    "            agents_observed_variables = {0: [0], 1: [1]}\n",
    "            game = {i: create_random_canonical_game(n_features, n_final_actions, n=1, m=0)\n",
    "                    for i in range(n_agents)}\n",
    "\n",
    "            env = TempNetMultiAgentEnv(\n",
    "                n_agents=n_agents,\n",
    "                n_features=n_features,\n",
    "                n_signaling_actions=n_signaling_actions,\n",
    "                n_final_actions=n_final_actions,\n",
    "                full_information=False,\n",
    "                game_dicts=game,\n",
    "                observed_variables=agents_observed_variables,\n",
    "                agent_type=TDLearningAgent,\n",
    "                graph=G\n",
    "            )\n",
    "\n",
    "            # Override agents manually with hyperparameters\n",
    "            env.agents = [\n",
    "                TDLearningAgent(\n",
    "                    n_actions=env.max_actions,\n",
    "                    learning_rate=params['learning_rate'],\n",
    "                    exploration_rate=params['exploration_rate'],\n",
    "                    exploration_decay=params['exploration_decay'],\n",
    "                    min_exploration_rate=params['min_exploration_rate']\n",
    "                ) for _ in range(n_agents)\n",
    "            ]\n",
    "\n",
    "            _, rewards_history, signal_information_history, _, _ = temp_simulation_function(\n",
    "                n_agents=n_agents,\n",
    "                n_features=n_features,\n",
    "                n_signaling_actions=n_signaling_actions,\n",
    "                n_final_actions=n_final_actions,\n",
    "                n_episodes=n_episodes,\n",
    "                with_signals=True,\n",
    "                plot=False,\n",
    "                env=env,\n",
    "                verbose=False\n",
    "            )\n",
    "\n",
    "            # Measure average reward in last 10% of episodes\n",
    "            final_rewards = [\n",
    "                np.mean(rewards[-n_episodes // 10:]) for rewards in rewards_history\n",
    "            ]\n",
    "            avg_rewards.append(np.mean(final_rewards))\n",
    "\n",
    "            # Measure final normalized mutual information\n",
    "            final_nmi = [\n",
    "                np.mean(agent_nmi[-n_episodes // 10:]) if len(agent_nmi) >= n_episodes // 10 else 0.0\n",
    "                for agent_nmi in signal_information_history\n",
    "            ]\n",
    "            avg_final_nmi.append(np.mean(final_nmi))\n",
    "\n",
    "        result = {\n",
    "            'params': params,\n",
    "            'mean_reward': np.mean(avg_rewards),\n",
    "            'std_reward': np.std(avg_rewards),\n",
    "            'mean_final_nmi': np.mean(avg_final_nmi)\n",
    "        }\n",
    "        results.append(result)\n",
    "        print(f\"Tested: {params} => Mean Final Reward: {result['mean_reward']:.3f}, Mean Final NMI: {result['mean_final_nmi']:.3f}, Std Reward: {result['std_reward']:.3f}\")\n",
    "\n",
    "    return sorted(results, key=lambda r: -r['mean_reward'])\n",
    "\n",
    "\n",
    "# Example usage\n",
    "param_grid = {\n",
    "    'learning_rate': [0.1, 0.05, 0.01],\n",
    "    'exploration_rate': [1.0, 0.75, 0.5,0.25],\n",
    "    'exploration_decay': [0.999,0.995, 0.99, 0.9],\n",
    "    'min_exploration_rate': [0.1,0.05,0.001, 0.0001]\n",
    "}\n",
    "\n",
    "search_results = parameter_search(param_grid, n_trials=100, n_episodes=5000)\n",
    "\n",
    "for r in search_results:\n",
    "    print(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472efd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df, top_configs = analyze_results(search_results, top_n=5)\n",
    "top_k_df = full_df.sort_values(by='mean_reward', ascending=False).head(10)\n",
    "plot_param_sensitivity(top_k_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
