{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52581437",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356d659d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imports import *\n",
    "from utils import *\n",
    "from agents import UrnAgent, QLearningAgent, TDLearningAgent\n",
    "from environment import NetMultiAgentEnv, TempNetMultiAgentEnv\n",
    "from simulation_function import simulation_function, temp_simulation_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dada83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_results(search_results, top_n=5, sort_by='mean_reward'):\n",
    "    \"\"\"\n",
    "    Takes parameter search results and shows the top configurations and summaries.\n",
    "    \"\"\"\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame([\n",
    "        {**res['params'], 'mean_reward': res['mean_reward'], 'std_reward': res['std_reward'], 'mean_final_nmi': res.get('mean_final_nmi', None)}\n",
    "        for res in search_results\n",
    "    ])\n",
    "\n",
    "    # Sort and show top-N\n",
    "    top_df = df.sort_values(by=sort_by, ascending=False).head(top_n)\n",
    "    print(f\"🔝 Top {top_n} Configurations by {sort_by}:\\n\")\n",
    "    print(top_df.to_string(index=False))\n",
    "\n",
    "    return df, top_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffc5d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_param_sensitivity(df, reward_col='mean_reward', error_col='std_reward', nmi_col='mean_final_nmi'):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    param_cols = [col for col in df.columns if col not in [reward_col, error_col, nmi_col]]\n",
    "\n",
    "    num_params = len(param_cols)\n",
    "    fig, axes = plt.subplots(num_params, 1, figsize=(8, 4 * num_params))\n",
    "\n",
    "    if num_params == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for i, param in enumerate(param_cols):\n",
    "        ax = axes[i]\n",
    "        grouped = df.groupby(param).agg({reward_col: 'mean', error_col: 'mean', nmi_col: 'mean'}).reset_index()\n",
    "\n",
    "        ax.errorbar(grouped[param], grouped[reward_col], yerr=grouped[error_col], fmt='o-', capsize=5, label='Mean Reward')\n",
    "        ax.set_xlabel(param)\n",
    "        ax.set_ylabel('Mean Reward')\n",
    "        ax.set_title(f'Effect of {param} on Reward')\n",
    "\n",
    "        if nmi_col in df.columns:\n",
    "            for j, row in grouped.iterrows():\n",
    "                ax.annotate(f\"NMI={row[nmi_col]:.2f}\", (row[param], row[reward_col]), fontsize=8)\n",
    "\n",
    "        ax.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_pairwise_performance(results):\n",
    "    df = pd.DataFrame([\n",
    "        {**r['params'], 'mean_reward': r['mean_reward'], 'mean_final_nmi': r['mean_final_nmi']}\n",
    "        for r in results\n",
    "    ])\n",
    "    sns.pairplot(df, diag_kind='kde', corner=True,\n",
    "                 plot_kws={'alpha': 0.7},\n",
    "                 hue=None)\n",
    "    plt.suptitle('Pairwise Parameter Exploration (Reward & NMI)', y=1.02)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dfe2b2",
   "metadata": {},
   "source": [
    "# QLearning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53603b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_search(\n",
    "    param_ranges,\n",
    "    n_simulations=20,\n",
    "    n_trials=10,\n",
    "    n_episodes=5000,\n",
    "    base_seed=42\n",
    "):\n",
    "    results = []\n",
    "\n",
    "    for sim_id in range(n_simulations):\n",
    "        seed = base_seed + sim_id\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "\n",
    "        # Sample one parameter set from ranges\n",
    "        params = {\n",
    "            k: random.uniform(*v) for k, v in param_ranges.items()\n",
    "        }\n",
    "\n",
    "        trial_rewards = []\n",
    "        trial_nmis = []\n",
    "\n",
    "        for trial in range(n_trials):\n",
    "            trial_seed = seed + trial * 1000\n",
    "            np.random.seed(trial_seed)\n",
    "            random.seed(trial_seed)\n",
    "\n",
    "            # Setup graph\n",
    "            G = nx.DiGraph()\n",
    "            G.add_nodes_from([0, 1])\n",
    "            G.add_edges_from([(0, 1), (1, 0)])\n",
    "\n",
    "            # Setup game and env\n",
    "            n_agents = 2\n",
    "            n_features = 2\n",
    "            n_signaling_actions = 2\n",
    "            n_final_actions = 4\n",
    "            agents_observed_variables = {0: [0], 1: [1]}\n",
    "            game = {i: create_random_canonical_game(n_features, n_final_actions, n=1, m=0)\n",
    "                    for i in range(n_agents)}\n",
    "\n",
    "            env = NetMultiAgentEnv(\n",
    "                n_agents=n_agents,\n",
    "                n_features=n_features,\n",
    "                n_signaling_actions=n_signaling_actions,\n",
    "                n_final_actions=n_final_actions,\n",
    "                full_information=False,\n",
    "                game_dicts=game,\n",
    "                observed_variables=agents_observed_variables,\n",
    "                agent_type=QLearningAgent,\n",
    "                initialize=False,\n",
    "                graph=G\n",
    "            )\n",
    "\n",
    "            # Override agents manually with hyperparameters\n",
    "            env.agents = [\n",
    "                QLearningAgent(\n",
    "                    n_signaling_actions=n_signaling_actions,\n",
    "                    n_final_actions=n_final_actions,\n",
    "                    exploration_rate=params['exploration_rate'],\n",
    "                    exploration_decay=params['exploration_decay'],\n",
    "                    min_exploration_rate=params['min_exploration_rate'],\n",
    "                    initialize=False\n",
    "                ) for _ in range(n_agents)\n",
    "            ]\n",
    "\n",
    "            _, rewards_history, signal_information_history, _, _ = simulation_function(\n",
    "                n_agents=n_agents,\n",
    "                n_features=n_features,\n",
    "                n_signaling_actions=n_signaling_actions,\n",
    "                n_final_actions=n_final_actions,\n",
    "                n_episodes=n_episodes,\n",
    "                with_signals=True,\n",
    "                plot=False,\n",
    "                env=env,\n",
    "                verbose=False\n",
    "            )\n",
    "\n",
    "            # Measure average reward in last 10% of episodes\n",
    "            final_rewards = [\n",
    "                np.mean(rewards[-n_episodes // 10:]) for rewards in rewards_history\n",
    "            ]\n",
    "            trial_rewards.append(np.mean(final_rewards))\n",
    "\n",
    "            # Measure final normalized mutual information\n",
    "            final_nmi = [\n",
    "                np.mean(agent_nmi[-n_episodes // 10:]) if len(agent_nmi) >= n_episodes // 10 else 0.0\n",
    "                for agent_nmi in signal_information_history\n",
    "            ]\n",
    "            trial_nmis.append(np.mean(final_nmi))\n",
    "\n",
    "        result = {\n",
    "            'params': params,\n",
    "            'mean_reward': np.mean(trial_rewards),\n",
    "            'std_reward': np.std(trial_rewards),\n",
    "            'mean_final_nmi': np.mean(trial_nmis)\n",
    "        }\n",
    "        results.append(result)\n",
    "        print(f\"Simulation {sim_id + 1}: {params} => Mean Reward: {result['mean_reward']:.3f}, Mean NMI: {result['mean_final_nmi']:.3f}, Std Reward: {result['std_reward']:.3f}\")\n",
    "\n",
    "    # Save to CSV\n",
    "    df = pd.DataFrame([\n",
    "        {**r['params'], 'mean_reward': r['mean_reward'], 'std_reward': r['std_reward'], 'mean_final_nmi': r['mean_final_nmi']}\n",
    "        for r in results\n",
    "    ])\n",
    "    save_path = f\"td_search_results_{int(time.time())}.csv\"\n",
    "    df.to_csv(save_path, index=False)\n",
    "    print(f\"Results saved to {save_path}\")\n",
    "\n",
    "    return sorted(results, key=lambda r: -r['mean_reward'])\n",
    "\n",
    "\n",
    "# Example usage\n",
    "param_ranges = {\n",
    "    'exploration_rate': (0.5, 1.0),\n",
    "    'exploration_decay': (0.95, 1.0),\n",
    "    'min_exploration_rate': (0.0001, 0.05)\n",
    "}\n",
    "\n",
    "q_search_results = parameter_search(param_ranges, n_simulations=100, n_trials=100, n_episodes=5000)\n",
    "\n",
    "for r in q_search_results:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5a3579",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pairwise_performance(q_search_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267c4137",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df, top_configs = analyze_results(q_search_results, top_n=15)\n",
    "top_k_df = full_df.sort_values(by='mean_reward', ascending=False).head(15)\n",
    "plot_param_sensitivity(top_k_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90afe00c",
   "metadata": {},
   "source": [
    "# TD Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212599ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_search(\n",
    "    param_ranges,\n",
    "    n_simulations=200,\n",
    "    n_trials=100,\n",
    "    n_episodes=5000,\n",
    "    base_seed=42\n",
    "):\n",
    "    results = []\n",
    "\n",
    "    for sim_id in range(n_simulations):\n",
    "        seed = base_seed + sim_id\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "\n",
    "        # Sample one parameter set from ranges\n",
    "        params = {\n",
    "            k: random.uniform(*v) for k, v in param_ranges.items()\n",
    "        }\n",
    "\n",
    "        trial_rewards = []\n",
    "        trial_nmis = []\n",
    "\n",
    "        for trial in range(n_trials):\n",
    "            trial_seed = seed + trial * 1000\n",
    "            np.random.seed(trial_seed)\n",
    "            random.seed(trial_seed)\n",
    "\n",
    "            # Setup graph\n",
    "            G = nx.DiGraph()\n",
    "            G.add_nodes_from([0, 1])\n",
    "            G.add_edges_from([(0, 1), (1, 0)])\n",
    "\n",
    "            # Setup game and env\n",
    "            n_agents = 2\n",
    "            n_features = 2\n",
    "            n_signaling_actions = 2\n",
    "            n_final_actions = 4\n",
    "            agents_observed_variables = {0: [0], 1: [1]}\n",
    "            game = {i: create_random_canonical_game(n_features, n_final_actions, n=1, m=0)\n",
    "                    for i in range(n_agents)}\n",
    "\n",
    "            env = TempNetMultiAgentEnv(\n",
    "                n_agents=n_agents,\n",
    "                n_features=n_features,\n",
    "                n_signaling_actions=n_signaling_actions,\n",
    "                n_final_actions=n_final_actions,\n",
    "                full_information=False,\n",
    "                game_dicts=game,\n",
    "                observed_variables=agents_observed_variables,\n",
    "                agent_type=TDLearningAgent,\n",
    "                graph=G\n",
    "            )\n",
    "\n",
    "            # Override agents manually with hyperparameters\n",
    "            env.agents = [\n",
    "                TDLearningAgent(\n",
    "                    n_actions=env.max_actions,\n",
    "                    learning_rate=params['learning_rate'],\n",
    "                    exploration_rate=params['exploration_rate'],\n",
    "                    exploration_decay=params['exploration_decay'],\n",
    "                    min_exploration_rate=params['min_exploration_rate']\n",
    "                ) for _ in range(n_agents)\n",
    "            ]\n",
    "\n",
    "            _, rewards_history, signal_information_history, _, _ = temp_simulation_function(\n",
    "                n_agents=n_agents,\n",
    "                n_features=n_features,\n",
    "                n_signaling_actions=n_signaling_actions,\n",
    "                n_final_actions=n_final_actions,\n",
    "                n_episodes=n_episodes,\n",
    "                with_signals=True,\n",
    "                plot=False,\n",
    "                env=env,\n",
    "                verbose=False\n",
    "            )\n",
    "\n",
    "            # Measure average reward in last 10% of episodes\n",
    "            final_rewards = [\n",
    "                np.mean(rewards[-n_episodes // 10:]) for rewards in rewards_history\n",
    "            ]\n",
    "            trial_rewards.append(np.mean(final_rewards))\n",
    "\n",
    "            # Measure final normalized mutual information\n",
    "            final_nmi = [\n",
    "                np.mean(agent_nmi[-n_episodes // 10:]) if len(agent_nmi) >= n_episodes // 10 else 0.0\n",
    "                for agent_nmi in signal_information_history\n",
    "            ]\n",
    "            trial_nmis.append(np.mean(final_nmi))\n",
    "\n",
    "        result = {\n",
    "            'params': params,\n",
    "            'mean_reward': np.mean(trial_rewards),\n",
    "            'std_reward': np.std(trial_rewards),\n",
    "            'mean_final_nmi': np.mean(trial_nmis)\n",
    "        }\n",
    "        results.append(result)\n",
    "        print(f\"Simulation {sim_id + 1}: {params} => Mean Reward: {result['mean_reward']:.3f}, Mean NMI: {result['mean_final_nmi']:.3f}, Std Reward: {result['std_reward']:.3f}\")\n",
    "\n",
    "    # Save to CSV\n",
    "    df = pd.DataFrame([\n",
    "        {**r['params'], 'mean_reward': r['mean_reward'], 'std_reward': r['std_reward'], 'mean_final_nmi': r['mean_final_nmi']}\n",
    "        for r in results\n",
    "    ])\n",
    "    save_path = f\"td_search_results_{int(time.time())}.csv\"\n",
    "    df.to_csv(save_path, index=False)\n",
    "    print(f\"Results saved to {save_path}\")\n",
    "    \n",
    "    return sorted(results, key=lambda r: -r['mean_reward'])\n",
    "\n",
    "def plot_pairwise_performance(results):\n",
    "    df = pd.DataFrame([\n",
    "        {**r['params'], 'mean_reward': r['mean_reward'], 'mean_final_nmi': r['mean_final_nmi']}\n",
    "        for r in results\n",
    "    ])\n",
    "    sns.pairplot(df, diag_kind='kde', corner=True,\n",
    "                 plot_kws={'alpha': 0.7},\n",
    "                 hue=None)\n",
    "    plt.suptitle('Pairwise Parameter Exploration (Reward & NMI)', y=1.02)\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "param_ranges = {\n",
    "    'learning_rate': (0.01, 0.2),\n",
    "    'exploration_rate': (0.25, 1.0),\n",
    "    'exploration_decay': (0.9, 1.0),\n",
    "    'min_exploration_rate': (0.0, 0.05)\n",
    "}\n",
    "\n",
    "td_search_results = parameter_search(param_ranges, n_simulations=200, n_trials=100, n_episodes=5000)\n",
    "\n",
    "\n",
    "for r in td_search_results:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3236880",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pairwise_performance(td_search_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472efd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df, top_configs = analyze_results(td_search_results, top_n=15)\n",
    "top_k_df = full_df.sort_values(by='mean_reward', ascending=False).head(15)\n",
    "plot_param_sensitivity(top_k_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
