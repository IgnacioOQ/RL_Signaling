{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52581437",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "356d659d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imports import *\n",
    "from utils import *\n",
    "from agents import UrnAgent, QLearningAgent, TDLearningAgent\n",
    "from environment import NetMultiAgentEnv, TempNetMultiAgentEnv\n",
    "from simulation_function import simulation_function, temp_simulation_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dada83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_results(search_results, top_n=5, sort_by='mean_reward'):\n",
    "    \"\"\"\n",
    "    Takes parameter search results and shows the top configurations and summaries.\n",
    "    \"\"\"\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame([\n",
    "        {**res['params'], 'mean_reward': res['mean_reward'], 'std_reward': res['std_reward'], 'mean_final_nmi': res.get('mean_final_nmi', None)}\n",
    "        for res in search_results\n",
    "    ])\n",
    "\n",
    "    # Sort and show top-N\n",
    "    top_df = df.sort_values(by=sort_by, ascending=False).head(top_n)\n",
    "    print(f\"🔝 Top {top_n} Configurations by {sort_by}:\\n\")\n",
    "    print(top_df.to_string(index=False))\n",
    "\n",
    "    return df, top_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ffc5d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_param_sensitivity(df, reward_col='mean_reward', error_col='std_reward', nmi_col='mean_final_nmi'):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    param_cols = [col for col in df.columns if col not in [reward_col, error_col, nmi_col]]\n",
    "\n",
    "    num_params = len(param_cols)\n",
    "    fig, axes = plt.subplots(num_params, 1, figsize=(8, 4 * num_params))\n",
    "\n",
    "    if num_params == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for i, param in enumerate(param_cols):\n",
    "        ax = axes[i]\n",
    "        grouped = df.groupby(param).agg({reward_col: 'mean', error_col: 'mean', nmi_col: 'mean'}).reset_index()\n",
    "\n",
    "        ax.errorbar(grouped[param], grouped[reward_col], yerr=grouped[error_col], fmt='o-', capsize=5, label='Mean Reward')\n",
    "        ax.set_xlabel(param)\n",
    "        ax.set_ylabel('Mean Reward')\n",
    "        ax.set_title(f'Effect of {param} on Reward')\n",
    "\n",
    "        if nmi_col in df.columns:\n",
    "            for j, row in grouped.iterrows():\n",
    "                ax.annotate(f\"NMI={row[nmi_col]:.2f}\", (row[param], row[reward_col]), fontsize=8)\n",
    "\n",
    "        ax.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_pairwise_performance(results):\n",
    "    df = pd.DataFrame([\n",
    "        {**r['params'], 'mean_reward': r['mean_reward'], 'mean_final_nmi': r['mean_final_nmi']}\n",
    "        for r in results\n",
    "    ])\n",
    "    sns.pairplot(df, diag_kind='kde', corner=True,\n",
    "                 plot_kws={'alpha': 0.7},\n",
    "                 hue=None)\n",
    "    plt.suptitle('Pairwise Parameter Exploration (Reward & NMI)', y=1.02)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dfe2b2",
   "metadata": {},
   "source": [
    "# QLearning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53603b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation 1: {'exploration_rate': 0.8197133992289418, 'exploration_decay': 0.9512505377611333, 'min_exploration_rate': 0.01382396298661905} => Mean Reward: 0.716, Mean NMI: 0.410, Std Reward: 0.161\n",
      "Simulation 2: {'exploration_rate': 0.51927591966869, 'exploration_decay': 0.9848112161318526, 'min_exploration_rate': 0.007282267747628515} => Mean Reward: 0.723, Mean NMI: 0.442, Std Reward: 0.172\n",
      "Simulation 3: {'exploration_rate': 0.7042679396272469, 'exploration_decay': 0.9770986004552298, 'min_exploration_rate': 0.043115627162315734} => Mean Reward: 0.723, Mean NMI: 0.392, Std Reward: 0.159\n",
      "Simulation 4: {'exploration_rate': 0.6359377071920453, 'exploration_decay': 0.9744014103925454, 'min_exploration_rate': 0.004185779184321212} => Mean Reward: 0.717, Mean NMI: 0.502, Std Reward: 0.175\n",
      "Simulation 5: {'exploration_rate': 0.944134038226244, 'exploration_decay': 0.9699997197985506, 'min_exploration_rate': 0.02947214712043418} => Mean Reward: 0.706, Mean NMI: 0.366, Std Reward: 0.173\n",
      "Simulation 6: {'exploration_rate': 0.6759231279139413, 'exploration_decay': 0.9715093122995049, 'min_exploration_rate': 0.022738176093119754} => Mean Reward: 0.752, Mean NMI: 0.444, Std Reward: 0.181\n",
      "Simulation 7: {'exploration_rate': 0.7740789142081648, 'exploration_decay': 0.9565945731687937, 'min_exploration_rate': 0.03560618742917442} => Mean Reward: 0.721, Mean NMI: 0.393, Std Reward: 0.167\n",
      "Simulation 8: {'exploration_rate': 0.5334419881713759, 'exploration_decay': 0.9706621408214129, 'min_exploration_rate': 0.005614186223912321} => Mean Reward: 0.681, Mean NMI: 0.386, Std Reward: 0.162\n",
      "Simulation 9: {'exploration_rate': 0.7487682843793011, 'exploration_decay': 0.963308686153627, 'min_exploration_rate': 0.03190681695604018} => Mean Reward: 0.717, Mean NMI: 0.394, Std Reward: 0.177\n",
      "Simulation 10: {'exploration_rate': 0.6217588813546685, 'exploration_decay': 0.9751104494751578, 'min_exploration_rate': 0.008180681963111216} => Mean Reward: 0.743, Mean NMI: 0.488, Std Reward: 0.169\n",
      "Simulation 11: {'exploration_rate': 0.9891774291854983, 'exploration_decay': 0.9527261327505229, 'min_exploration_rate': 0.036310519886598815} => Mean Reward: 0.692, Mean NMI: 0.337, Std Reward: 0.162\n",
      "Simulation 12: {'exploration_rate': 0.8085707383349761, 'exploration_decay': 0.9945069276601275, 'min_exploration_rate': 0.02287871226782337} => Mean Reward: 0.709, Mean NMI: 0.351, Std Reward: 0.157\n",
      "Simulation 13: {'exploration_rate': 0.9568214862774456, 'exploration_decay': 0.9568739009793615, 'min_exploration_rate': 0.027847899799544332} => Mean Reward: 0.746, Mean NMI: 0.424, Std Reward: 0.176\n",
      "Simulation 14: {'exploration_rate': 0.5451699271346748, 'exploration_decay': 0.9975316782281722, 'min_exploration_rate': 0.007583555544499674} => Mean Reward: 0.740, Mean NMI: 0.388, Std Reward: 0.174\n",
      "Simulation 15: {'exploration_rate': 0.982959594012472, 'exploration_decay': 0.9779877956999452, 'min_exploration_rate': 0.023736227221711117} => Mean Reward: 0.754, Mean NMI: 0.461, Std Reward: 0.170\n",
      "Simulation 16: {'exploration_rate': 0.5212828567912884, 'exploration_decay': 0.9794843225200827, 'min_exploration_rate': 0.0010636094862246056} => Mean Reward: 0.648, Mean NMI: 0.450, Std Reward: 0.154\n",
      "Simulation 17: {'exploration_rate': 0.7903785824669783, 'exploration_decay': 0.960286199949054, 'min_exploration_rate': 0.03213427144978364} => Mean Reward: 0.719, Mean NMI: 0.387, Std Reward: 0.171\n",
      "Simulation 18: {'exploration_rate': 0.6120927374236632, 'exploration_decay': 0.9966320148298674, 'min_exploration_rate': 0.04729260608501469} => Mean Reward: 0.715, Mean NMI: 0.358, Std Reward: 0.163\n",
      "Simulation 19: {'exploration_rate': 0.6539033773668887, 'exploration_decay': 0.9788978938587519, 'min_exploration_rate': 0.01335433996736697} => Mean Reward: 0.750, Mean NMI: 0.470, Std Reward: 0.177\n",
      "Simulation 20: {'exploration_rate': 0.7472437179897173, 'exploration_decay': 0.97783458650737, 'min_exploration_rate': 0.04050616349921432} => Mean Reward: 0.699, Mean NMI: 0.361, Std Reward: 0.147\n",
      "Simulation 21: {'exploration_rate': 0.9639957829388371, 'exploration_decay': 0.9586513445024022, 'min_exploration_rate': 0.04853438327206233} => Mean Reward: 0.707, Mean NMI: 0.350, Std Reward: 0.172\n",
      "Simulation 22: {'exploration_rate': 0.7225696125784113, 'exploration_decay': 0.9647044241169442, 'min_exploration_rate': 0.04548050081522228} => Mean Reward: 0.689, Mean NMI: 0.335, Std Reward: 0.148\n",
      "Simulation 23: {'exploration_rate': 0.7380791311146728, 'exploration_decay': 0.9815109527159442, 'min_exploration_rate': 0.0202576523814668} => Mean Reward: 0.749, Mean NMI: 0.460, Std Reward: 0.166\n",
      "Simulation 24: {'exploration_rate': 0.7073719996543395, 'exploration_decay': 0.9643850086309906, 'min_exploration_rate': 0.042251145814920794} => Mean Reward: 0.720, Mean NMI: 0.359, Std Reward: 0.172\n",
      "Simulation 25: {'exploration_rate': 0.5354044635295363, 'exploration_decay': 0.9717293876870671, 'min_exploration_rate': 0.012387863573945992} => Mean Reward: 0.711, Mean NMI: 0.382, Std Reward: 0.178\n",
      "Simulation 26: {'exploration_rate': 0.5373968784365554, 'exploration_decay': 0.9883000367911107, 'min_exploration_rate': 0.04860747401293439} => Mean Reward: 0.683, Mean NMI: 0.335, Std Reward: 0.171\n",
      "Simulation 27: {'exploration_rate': 0.8708344678072866, 'exploration_decay': 0.9970401941573608, 'min_exploration_rate': 0.03739542950302272} => Mean Reward: 0.706, Mean NMI: 0.333, Std Reward: 0.173\n",
      "Simulation 28: {'exploration_rate': 0.8421204762060366, 'exploration_decay': 0.9547918654238402, 'min_exploration_rate': 0.008429048832707974} => Mean Reward: 0.723, Mean NMI: 0.453, Std Reward: 0.161\n",
      "Simulation 29: {'exploration_rate': 0.954884618961936, 'exploration_decay': 0.9647039520379339, 'min_exploration_rate': 0.02302575057862785} => Mean Reward: 0.718, Mean NMI: 0.412, Std Reward: 0.173\n",
      "Simulation 30: {'exploration_rate': 0.6618766729103829, 'exploration_decay': 0.9810014742607772, 'min_exploration_rate': 0.0005231519745294328} => Mean Reward: 0.706, Mean NMI: 0.526, Std Reward: 0.160\n",
      "Simulation 31: {'exploration_rate': 0.5367133274011657, 'exploration_decay': 0.9797180964568118, 'min_exploration_rate': 0.049473403392692725} => Mean Reward: 0.695, Mean NMI: 0.356, Std Reward: 0.158\n",
      "Simulation 32: {'exploration_rate': 0.6399077367864939, 'exploration_decay': 0.9751070221812367, 'min_exploration_rate': 0.028250976041991158} => Mean Reward: 0.742, Mean NMI: 0.424, Std Reward: 0.179\n",
      "Simulation 33: {'exploration_rate': 0.9298642225959327, 'exploration_decay': 0.9757639914256561, 'min_exploration_rate': 0.005769178126988279} => Mean Reward: 0.717, Mean NMI: 0.442, Std Reward: 0.171\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 120\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m    114\u001b[0m param_ranges \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexploration_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: (\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m1.0\u001b[39m),\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexploration_decay\u001b[39m\u001b[38;5;124m'\u001b[39m: (\u001b[38;5;241m0.95\u001b[39m, \u001b[38;5;241m1.0\u001b[39m),\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin_exploration_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: (\u001b[38;5;241m0.0001\u001b[39m, \u001b[38;5;241m0.05\u001b[39m)\n\u001b[0;32m    118\u001b[0m }\n\u001b[1;32m--> 120\u001b[0m q_search_results \u001b[38;5;241m=\u001b[39m parameter_search(param_ranges, n_simulations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, n_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m)\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m q_search_results:\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;28mprint\u001b[39m(r)\n",
      "Cell \u001b[1;32mIn[4], line 67\u001b[0m, in \u001b[0;36mparameter_search\u001b[1;34m(param_ranges, n_simulations, n_trials, n_episodes, base_seed)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Override agents manually with hyperparameters\u001b[39;00m\n\u001b[0;32m     56\u001b[0m env\u001b[38;5;241m.\u001b[39magents \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     57\u001b[0m     QLearningAgent(\n\u001b[0;32m     58\u001b[0m         n_signaling_actions\u001b[38;5;241m=\u001b[39mn_signaling_actions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     64\u001b[0m     ) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_agents)\n\u001b[0;32m     65\u001b[0m ]\n\u001b[1;32m---> 67\u001b[0m _, rewards_history, signal_information_history, _, _ \u001b[38;5;241m=\u001b[39m simulation_function(\n\u001b[0;32m     68\u001b[0m     n_agents\u001b[38;5;241m=\u001b[39mn_agents,\n\u001b[0;32m     69\u001b[0m     n_features\u001b[38;5;241m=\u001b[39mn_features,\n\u001b[0;32m     70\u001b[0m     n_signaling_actions\u001b[38;5;241m=\u001b[39mn_signaling_actions,\n\u001b[0;32m     71\u001b[0m     n_final_actions\u001b[38;5;241m=\u001b[39mn_final_actions,\n\u001b[0;32m     72\u001b[0m     n_episodes\u001b[38;5;241m=\u001b[39mn_episodes,\n\u001b[0;32m     73\u001b[0m     with_signals\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     74\u001b[0m     plot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     75\u001b[0m     env\u001b[38;5;241m=\u001b[39menv,\n\u001b[0;32m     76\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     77\u001b[0m )\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# Measure average reward in last 10% of episodes\u001b[39;00m\n\u001b[0;32m     80\u001b[0m final_rewards \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     81\u001b[0m     np\u001b[38;5;241m.\u001b[39mmean(rewards[\u001b[38;5;241m-\u001b[39mn_episodes \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m10\u001b[39m:]) \u001b[38;5;28;01mfor\u001b[39;00m rewards \u001b[38;5;129;01min\u001b[39;00m rewards_history\n\u001b[0;32m     82\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\LokalAdmin\\Documents\\GitRepositories\\RL_Signaling\\simulation_function.py:58\u001b[0m, in \u001b[0;36msimulation_function\u001b[1;34m(n_agents, n_features, n_signaling_actions, n_final_actions, n_episodes, with_signals, plot, env, verbose)\u001b[0m\n\u001b[0;32m     55\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menvironment step is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv\u001b[38;5;241m.\u001b[39mcurrent_step\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Step 3: Agents choose final actions based new observations\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m final_actions \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mget_actions(new_observations)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[0;32m     60\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124magents final_actions are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal_actions\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\LokalAdmin\\Documents\\GitRepositories\\RL_Signaling\\environment.py:149\u001b[0m, in \u001b[0;36mNetMultiAgentEnv.get_actions\u001b[1;34m(self, agents_observations)\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m, agents_observations):\n\u001b[1;32m--> 149\u001b[0m     final_actions \u001b[38;5;241m=\u001b[39m [agent\u001b[38;5;241m.\u001b[39mget_action(observation) \u001b[38;5;28;01mfor\u001b[39;00m agent, observation \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magents, agents_observations)]\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;66;03m# update action usage tracking\u001b[39;00m\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_agents):\n",
      "File \u001b[1;32mc:\\Users\\LokalAdmin\\Documents\\GitRepositories\\RL_Signaling\\agents.py:173\u001b[0m, in \u001b[0;36mQLearningAgent.get_action\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m    171\u001b[0m     action \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_final_actions \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 173\u001b[0m     action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_table_action[state])\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_counts[state][action] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m action\n",
      "File \u001b[1;32mc:\\Users\\LokalAdmin\\anaconda3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:1229\u001b[0m, in \u001b[0;36margmax\u001b[1;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[0;32m   1142\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1143\u001b[0m \u001b[38;5;124;03mReturns the indices of the maximum values along an axis.\u001b[39;00m\n\u001b[0;32m   1144\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1226\u001b[0m \u001b[38;5;124;03m(2, 1, 4)\u001b[39;00m\n\u001b[0;32m   1227\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1228\u001b[0m kwds \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeepdims\u001b[39m\u001b[38;5;124m'\u001b[39m: keepdims} \u001b[38;5;28;01mif\u001b[39;00m keepdims \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39m_NoValue \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m-> 1229\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _wrapfunc(a, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124margmax\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[1;32mc:\\Users\\LokalAdmin\\anaconda3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:59\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bound(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def parameter_search(\n",
    "    param_ranges,\n",
    "    n_simulations=20,\n",
    "    n_trials=10,\n",
    "    n_episodes=5000,\n",
    "    base_seed=42\n",
    "):\n",
    "    results = []\n",
    "\n",
    "    for sim_id in range(n_simulations):\n",
    "        seed = base_seed + sim_id\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "\n",
    "        # Sample one parameter set from ranges\n",
    "        params = {\n",
    "            k: random.uniform(*v) for k, v in param_ranges.items()\n",
    "        }\n",
    "\n",
    "        trial_rewards = []\n",
    "        trial_nmis = []\n",
    "\n",
    "        for trial in range(n_trials):\n",
    "            trial_seed = seed + trial * 1000\n",
    "            np.random.seed(trial_seed)\n",
    "            random.seed(trial_seed)\n",
    "\n",
    "            # Setup graph\n",
    "            G = nx.DiGraph()\n",
    "            G.add_nodes_from([0, 1])\n",
    "            G.add_edges_from([(0, 1), (1, 0)])\n",
    "\n",
    "            # Setup game and env\n",
    "            n_agents = 2\n",
    "            n_features = 2\n",
    "            n_signaling_actions = 2\n",
    "            n_final_actions = 4\n",
    "            agents_observed_variables = {0: [0], 1: [1]}\n",
    "            game = {i: create_random_canonical_game(n_features, n_final_actions, n=1, m=0)\n",
    "                    for i in range(n_agents)}\n",
    "\n",
    "            env = NetMultiAgentEnv(\n",
    "                n_agents=n_agents,\n",
    "                n_features=n_features,\n",
    "                n_signaling_actions=n_signaling_actions,\n",
    "                n_final_actions=n_final_actions,\n",
    "                full_information=False,\n",
    "                game_dicts=game,\n",
    "                observed_variables=agents_observed_variables,\n",
    "                agent_type=QLearningAgent,\n",
    "                initialize=False,\n",
    "                graph=G\n",
    "            )\n",
    "\n",
    "            # Override agents manually with hyperparameters\n",
    "            env.agents = [\n",
    "                QLearningAgent(\n",
    "                    n_signaling_actions=n_signaling_actions,\n",
    "                    n_final_actions=n_final_actions,\n",
    "                    exploration_rate=params['exploration_rate'],\n",
    "                    exploration_decay=params['exploration_decay'],\n",
    "                    min_exploration_rate=params['min_exploration_rate'],\n",
    "                    initialize=False\n",
    "                ) for _ in range(n_agents)\n",
    "            ]\n",
    "\n",
    "            _, rewards_history, signal_information_history, _, _ = simulation_function(\n",
    "                n_agents=n_agents,\n",
    "                n_features=n_features,\n",
    "                n_signaling_actions=n_signaling_actions,\n",
    "                n_final_actions=n_final_actions,\n",
    "                n_episodes=n_episodes,\n",
    "                with_signals=True,\n",
    "                plot=False,\n",
    "                env=env,\n",
    "                verbose=False\n",
    "            )\n",
    "\n",
    "            # Measure average reward in last 10% of episodes\n",
    "            final_rewards = [\n",
    "                np.mean(rewards[-n_episodes // 10:]) for rewards in rewards_history\n",
    "            ]\n",
    "            trial_rewards.append(np.mean(final_rewards))\n",
    "\n",
    "            # Measure final normalized mutual information\n",
    "            final_nmi = [\n",
    "                np.mean(agent_nmi[-n_episodes // 10:]) if len(agent_nmi) >= n_episodes // 10 else 0.0\n",
    "                for agent_nmi in signal_information_history\n",
    "            ]\n",
    "            trial_nmis.append(np.mean(final_nmi))\n",
    "\n",
    "        result = {\n",
    "            'params': params,\n",
    "            'mean_reward': np.mean(trial_rewards),\n",
    "            'std_reward': np.std(trial_rewards),\n",
    "            'mean_final_nmi': np.mean(trial_nmis)\n",
    "        }\n",
    "        results.append(result)\n",
    "        print(f\"Simulation {sim_id + 1}: {params} => Mean Reward: {result['mean_reward']:.3f}, Mean NMI: {result['mean_final_nmi']:.3f}, Std Reward: {result['std_reward']:.3f}\")\n",
    "\n",
    "    # Save to CSV\n",
    "    df = pd.DataFrame([\n",
    "        {**r['params'], 'mean_reward': r['mean_reward'], 'std_reward': r['std_reward'], 'mean_final_nmi': r['mean_final_nmi']}\n",
    "        for r in results\n",
    "    ])\n",
    "    save_path = f\"td_search_results_{int(time.time())}.csv\"\n",
    "    df.to_csv(save_path, index=False)\n",
    "    print(f\"Results saved to {save_path}\")\n",
    "\n",
    "    return sorted(results, key=lambda r: -r['mean_reward'])\n",
    "\n",
    "\n",
    "# Example usage\n",
    "param_ranges = {\n",
    "    'exploration_rate': (0.5, 1.0),\n",
    "    'exploration_decay': (0.95, 1.0),\n",
    "    'min_exploration_rate': (0.0001, 0.05)\n",
    "}\n",
    "\n",
    "q_search_results = parameter_search(param_ranges, n_simulations=100, n_trials=100, n_episodes=5000)\n",
    "\n",
    "for r in q_search_results:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5a3579",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pairwise_performance(q_search_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267c4137",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df, top_configs = analyze_results(q_search_results, top_n=15)\n",
    "top_k_df = full_df.sort_values(by='mean_reward', ascending=False).head(15)\n",
    "plot_param_sensitivity(top_k_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90afe00c",
   "metadata": {},
   "source": [
    "# TD Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212599ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_search(\n",
    "    param_ranges,\n",
    "    n_simulations=200,\n",
    "    n_trials=100,\n",
    "    n_episodes=5000,\n",
    "    base_seed=42\n",
    "):\n",
    "    results = []\n",
    "\n",
    "    for sim_id in range(n_simulations):\n",
    "        seed = base_seed + sim_id\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "\n",
    "        # Sample one parameter set from ranges\n",
    "        params = {\n",
    "            k: random.uniform(*v) for k, v in param_ranges.items()\n",
    "        }\n",
    "\n",
    "        trial_rewards = []\n",
    "        trial_nmis = []\n",
    "\n",
    "        for trial in range(n_trials):\n",
    "            trial_seed = seed + trial * 1000\n",
    "            np.random.seed(trial_seed)\n",
    "            random.seed(trial_seed)\n",
    "\n",
    "            # Setup graph\n",
    "            G = nx.DiGraph()\n",
    "            G.add_nodes_from([0, 1])\n",
    "            G.add_edges_from([(0, 1), (1, 0)])\n",
    "\n",
    "            # Setup game and env\n",
    "            n_agents = 2\n",
    "            n_features = 2\n",
    "            n_signaling_actions = 2\n",
    "            n_final_actions = 4\n",
    "            agents_observed_variables = {0: [0], 1: [1]}\n",
    "            game = {i: create_random_canonical_game(n_features, n_final_actions, n=1, m=0)\n",
    "                    for i in range(n_agents)}\n",
    "\n",
    "            env = TempNetMultiAgentEnv(\n",
    "                n_agents=n_agents,\n",
    "                n_features=n_features,\n",
    "                n_signaling_actions=n_signaling_actions,\n",
    "                n_final_actions=n_final_actions,\n",
    "                full_information=False,\n",
    "                game_dicts=game,\n",
    "                observed_variables=agents_observed_variables,\n",
    "                agent_type=TDLearningAgent,\n",
    "                graph=G\n",
    "            )\n",
    "\n",
    "            # Override agents manually with hyperparameters\n",
    "            env.agents = [\n",
    "                TDLearningAgent(\n",
    "                    n_actions=env.max_actions,\n",
    "                    learning_rate=params['learning_rate'],\n",
    "                    exploration_rate=params['exploration_rate'],\n",
    "                    exploration_decay=params['exploration_decay'],\n",
    "                    min_exploration_rate=params['min_exploration_rate']\n",
    "                ) for _ in range(n_agents)\n",
    "            ]\n",
    "\n",
    "            _, rewards_history, signal_information_history, _, _ = temp_simulation_function(\n",
    "                n_agents=n_agents,\n",
    "                n_features=n_features,\n",
    "                n_signaling_actions=n_signaling_actions,\n",
    "                n_final_actions=n_final_actions,\n",
    "                n_episodes=n_episodes,\n",
    "                with_signals=True,\n",
    "                plot=False,\n",
    "                env=env,\n",
    "                verbose=False\n",
    "            )\n",
    "\n",
    "            # Measure average reward in last 10% of episodes\n",
    "            final_rewards = [\n",
    "                np.mean(rewards[-n_episodes // 10:]) for rewards in rewards_history\n",
    "            ]\n",
    "            trial_rewards.append(np.mean(final_rewards))\n",
    "\n",
    "            # Measure final normalized mutual information\n",
    "            final_nmi = [\n",
    "                np.mean(agent_nmi[-n_episodes // 10:]) if len(agent_nmi) >= n_episodes // 10 else 0.0\n",
    "                for agent_nmi in signal_information_history\n",
    "            ]\n",
    "            trial_nmis.append(np.mean(final_nmi))\n",
    "\n",
    "        result = {\n",
    "            'params': params,\n",
    "            'mean_reward': np.mean(trial_rewards),\n",
    "            'std_reward': np.std(trial_rewards),\n",
    "            'mean_final_nmi': np.mean(trial_nmis)\n",
    "        }\n",
    "        results.append(result)\n",
    "        print(f\"Simulation {sim_id + 1}: {params} => Mean Reward: {result['mean_reward']:.3f}, Mean NMI: {result['mean_final_nmi']:.3f}, Std Reward: {result['std_reward']:.3f}\")\n",
    "\n",
    "    # Save to CSV\n",
    "    df = pd.DataFrame([\n",
    "        {**r['params'], 'mean_reward': r['mean_reward'], 'std_reward': r['std_reward'], 'mean_final_nmi': r['mean_final_nmi']}\n",
    "        for r in results\n",
    "    ])\n",
    "    save_path = f\"td_search_results_{int(time.time())}.csv\"\n",
    "    df.to_csv(save_path, index=False)\n",
    "    print(f\"Results saved to {save_path}\")\n",
    "    \n",
    "    return sorted(results, key=lambda r: -r['mean_reward'])\n",
    "\n",
    "def plot_pairwise_performance(results):\n",
    "    df = pd.DataFrame([\n",
    "        {**r['params'], 'mean_reward': r['mean_reward'], 'mean_final_nmi': r['mean_final_nmi']}\n",
    "        for r in results\n",
    "    ])\n",
    "    sns.pairplot(df, diag_kind='kde', corner=True,\n",
    "                 plot_kws={'alpha': 0.7},\n",
    "                 hue=None)\n",
    "    plt.suptitle('Pairwise Parameter Exploration (Reward & NMI)', y=1.02)\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "param_ranges = {\n",
    "    'learning_rate': (0.01, 0.2),\n",
    "    'exploration_rate': (0.25, 1.0),\n",
    "    'exploration_decay': (0.9, 1.0),\n",
    "    'min_exploration_rate': (0.0, 0.05)\n",
    "}\n",
    "\n",
    "td_search_results = parameter_search(param_ranges, n_simulations=200, n_trials=100, n_episodes=5000)\n",
    "\n",
    "\n",
    "for r in td_search_results:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3236880",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pairwise_performance(td_search_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472efd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df, top_configs = analyze_results(td_search_results, top_n=15)\n",
    "top_k_df = full_df.sort_values(by='mean_reward', ascending=False).head(15)\n",
    "plot_param_sensitivity(top_k_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
