{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52581437",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "356d659d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imports import *\n",
    "from utils import *\n",
    "from agents import UrnAgent, QLearningAgent, TDLearningAgent\n",
    "from environment import NetMultiAgentEnv, TempNetMultiAgentEnv\n",
    "from simulation_function import simulation_function, temp_simulation_function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dfe2b2",
   "metadata": {},
   "source": [
    "# QLearning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80eeec9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tested: {'exploration_rate': 1.0, 'exploration_decay': 0.995, 'min_exploration_rate': 0.001} => Mean Final Reward: 0.999, Mean Final NMI: 0.837\n",
      "Tested: {'exploration_rate': 1.0, 'exploration_decay': 0.995, 'min_exploration_rate': 0.01} => Mean Final Reward: 0.745, Mean Final NMI: 0.374\n",
      "Tested: {'exploration_rate': 1.0, 'exploration_decay': 0.99, 'min_exploration_rate': 0.001} => Mean Final Reward: 0.754, Mean Final NMI: 0.446\n",
      "Tested: {'exploration_rate': 1.0, 'exploration_decay': 0.99, 'min_exploration_rate': 0.01} => Mean Final Reward: 0.986, Mean Final NMI: 0.888\n",
      "{'params': {'exploration_rate': 1.0, 'exploration_decay': 0.995, 'min_exploration_rate': 0.001}, 'mean_reward': 0.999, 'std_reward': 0.0, 'mean_final_nmi': 0.8365360108951392}\n",
      "{'params': {'exploration_rate': 1.0, 'exploration_decay': 0.99, 'min_exploration_rate': 0.01}, 'mean_reward': 0.9855, 'std_reward': 0.0015000000000000013, 'mean_final_nmi': 0.8876236179061014}\n",
      "{'params': {'exploration_rate': 1.0, 'exploration_decay': 0.99, 'min_exploration_rate': 0.001}, 'mean_reward': 0.7545, 'std_reward': 0.005500000000000005, 'mean_final_nmi': 0.44600481263377306}\n",
      "{'params': {'exploration_rate': 1.0, 'exploration_decay': 0.995, 'min_exploration_rate': 0.01}, 'mean_reward': 0.745, 'std_reward': 0.0010000000000000009, 'mean_final_nmi': 0.37421650662683015}\n"
     ]
    }
   ],
   "source": [
    "def parameter_search(\n",
    "    param_grid,\n",
    "    n_trials=3,\n",
    "    n_episodes=10000,\n",
    "    base_seed=42\n",
    "):\n",
    "    results = []\n",
    "\n",
    "    # Generate all combinations of hyperparameters\n",
    "    keys = list(param_grid.keys())\n",
    "    combos = list(itertools.product(*[param_grid[k] for k in keys]))\n",
    "\n",
    "    for combo in combos:\n",
    "        params = dict(zip(keys, combo))\n",
    "        avg_rewards = []\n",
    "        avg_final_nmi = []\n",
    "\n",
    "        for trial in range(n_trials):\n",
    "            seed = base_seed + trial\n",
    "            np.random.seed(seed)\n",
    "\n",
    "            # Setup graph\n",
    "            G = nx.DiGraph()\n",
    "            G.add_nodes_from([0, 1])\n",
    "            G.add_edges_from([(0, 1), (1, 0)])\n",
    "\n",
    "            # Setup game and env\n",
    "            n_agents = 2\n",
    "            n_features = 2\n",
    "            n_signaling_actions = 2\n",
    "            n_final_actions = 4\n",
    "            agents_observed_variables = {0: [0], 1: [1]}\n",
    "            game = {i: create_random_canonical_game(n_features, n_final_actions, n=1, m=0)\n",
    "                    for i in range(n_agents)}\n",
    "\n",
    "            env = NetMultiAgentEnv(\n",
    "                n_agents=n_agents,\n",
    "                n_features=n_features,\n",
    "                n_signaling_actions=n_signaling_actions,\n",
    "                n_final_actions=n_final_actions,\n",
    "                full_information=False,\n",
    "                game_dicts=game,\n",
    "                observed_variables=agents_observed_variables,\n",
    "                agent_type=QLearningAgent,\n",
    "                initialize=False,\n",
    "                graph=G\n",
    "            )\n",
    "\n",
    "            # Override agents manually with hyperparameters\n",
    "            env.agents = [\n",
    "                QLearningAgent(\n",
    "                    n_signaling_actions=n_signaling_actions,\n",
    "                    n_final_actions=n_final_actions,\n",
    "                    exploration_rate=params['exploration_rate'],\n",
    "                    exploration_decay=params['exploration_decay'],\n",
    "                    min_exploration_rate=params['min_exploration_rate'],\n",
    "                    initialize=False\n",
    "                ) for _ in range(n_agents)\n",
    "            ]\n",
    "\n",
    "            _, rewards_history, signal_information_history, _, _ = simulation_function(\n",
    "                n_agents=n_agents,\n",
    "                n_features=n_features,\n",
    "                n_signaling_actions=n_signaling_actions,\n",
    "                n_final_actions=n_final_actions,\n",
    "                n_episodes=n_episodes,\n",
    "                with_signals=True,\n",
    "                plot=False,\n",
    "                env=env,\n",
    "                verbose=False\n",
    "            )\n",
    "\n",
    "            # Measure average reward in last 10% of episodes\n",
    "            final_rewards = [\n",
    "                np.mean(rewards[-n_episodes // 10:]) for rewards in rewards_history\n",
    "            ]\n",
    "            avg_rewards.append(np.mean(final_rewards))\n",
    "\n",
    "            # Measure final normalized mutual information\n",
    "            final_nmi = [\n",
    "                np.mean(agent_nmi[-n_episodes // 10:]) if len(agent_nmi) >= n_episodes // 10 else 0.0\n",
    "                for agent_nmi in signal_information_history\n",
    "            ]\n",
    "            avg_final_nmi.append(np.mean(final_nmi))\n",
    "\n",
    "        result = {\n",
    "            'params': params,\n",
    "            'mean_reward': np.mean(avg_rewards),\n",
    "            'std_reward': np.std(avg_rewards),\n",
    "            'mean_final_nmi': np.mean(avg_final_nmi)\n",
    "        }\n",
    "        results.append(result)\n",
    "        print(f\"Tested: {params} => Mean Final Reward: {result['mean_reward']:.3f}, Mean Final NMI: {result['mean_final_nmi']:.3f}\")\n",
    "\n",
    "    return sorted(results, key=lambda r: -r['mean_reward'])\n",
    "\n",
    "\n",
    "# Example usage\n",
    "param_grid = {\n",
    "    'exploration_rate': [1.0, 0.5],\n",
    "    'exploration_decay': [0.995, 0.90, 0.9],\n",
    "    'min_exploration_rate': [0.001, 0.0001]\n",
    "}\n",
    "\n",
    "search_results = parameter_search(param_grid, n_trials=100, n_episodes=5000)\n",
    "\n",
    "for r in search_results:\n",
    "    print(r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90afe00c",
   "metadata": {},
   "source": [
    "# TD Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80987ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tested: {'learning_rate': 0.1, 'exploration_rate': 1.0, 'exploration_decay': 0.995, 'min_exploration_rate': 0.001} => Mean Final Reward: 0.841, Mean Final NMI: 0.548\n",
      "Tested: {'learning_rate': 0.1, 'exploration_rate': 1.0, 'exploration_decay': 0.995, 'min_exploration_rate': 0.0001} => Mean Final Reward: 0.754, Mean Final NMI: 0.327\n",
      "Tested: {'learning_rate': 0.1, 'exploration_rate': 1.0, 'exploration_decay': 0.9, 'min_exploration_rate': 0.001} => Mean Final Reward: 0.432, Mean Final NMI: 0.180\n",
      "Tested: {'learning_rate': 0.1, 'exploration_rate': 1.0, 'exploration_decay': 0.9, 'min_exploration_rate': 0.0001} => Mean Final Reward: 0.372, Mean Final NMI: 0.198\n",
      "Tested: {'learning_rate': 0.1, 'exploration_rate': 1.0, 'exploration_decay': 0.9, 'min_exploration_rate': 0.001} => Mean Final Reward: 0.452, Mean Final NMI: 0.327\n",
      "Tested: {'learning_rate': 0.1, 'exploration_rate': 1.0, 'exploration_decay': 0.9, 'min_exploration_rate': 0.0001} => Mean Final Reward: 0.358, Mean Final NMI: 0.273\n",
      "Tested: {'learning_rate': 0.1, 'exploration_rate': 0.5, 'exploration_decay': 0.995, 'min_exploration_rate': 0.001} => Mean Final Reward: 0.672, Mean Final NMI: 0.322\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 104\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m     97\u001b[0m param_grid \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.05\u001b[39m, \u001b[38;5;241m0.01\u001b[39m],\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexploration_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m0.5\u001b[39m],\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexploration_decay\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m0.995\u001b[39m, \u001b[38;5;241m0.90\u001b[39m, \u001b[38;5;241m0.9\u001b[39m],\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin_exploration_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m0.001\u001b[39m, \u001b[38;5;241m0.0001\u001b[39m]\n\u001b[1;32m    102\u001b[0m }\n\u001b[0;32m--> 104\u001b[0m search_results \u001b[38;5;241m=\u001b[39m \u001b[43mparameter_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m search_results:\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28mprint\u001b[39m(r)\n",
      "Cell \u001b[0;32mIn[7], line 59\u001b[0m, in \u001b[0;36mparameter_search\u001b[0;34m(param_grid, n_trials, n_episodes, base_seed)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Override agents manually with hyperparameters\u001b[39;00m\n\u001b[1;32m     49\u001b[0m env\u001b[38;5;241m.\u001b[39magents \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     50\u001b[0m     TDLearningAgent(\n\u001b[1;32m     51\u001b[0m         n_actions\u001b[38;5;241m=\u001b[39menv\u001b[38;5;241m.\u001b[39mmax_actions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     56\u001b[0m     ) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_agents)\n\u001b[1;32m     57\u001b[0m ]\n\u001b[0;32m---> 59\u001b[0m _, rewards_history, signal_information_history, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtemp_simulation_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_agents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_agents\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_signaling_actions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_signaling_actions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_final_actions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_final_actions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_episodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwith_signals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m     69\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Measure average reward in last 10% of episodes\u001b[39;00m\n\u001b[1;32m     72\u001b[0m final_rewards \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     73\u001b[0m     np\u001b[38;5;241m.\u001b[39mmean(rewards[\u001b[38;5;241m-\u001b[39mn_episodes \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m10\u001b[39m:]) \u001b[38;5;28;01mfor\u001b[39;00m rewards \u001b[38;5;129;01min\u001b[39;00m rewards_history\n\u001b[1;32m     74\u001b[0m ]\n",
      "File \u001b[0;32m~/Documents/VS Code/GitHub Repositories/RL_Signaling/simulation_function.py:249\u001b[0m, in \u001b[0;36mtemp_simulation_function\u001b[0;34m(n_agents, n_features, n_signaling_actions, n_final_actions, n_episodes, with_signals, plot, env, verbose)\u001b[0m\n\u001b[1;32m    245\u001b[0m rewards, done \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mplay_step(final_actions)\n\u001b[1;32m    247\u001b[0m \u001b[38;5;66;03m# Step 5: Update agent knowledge\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# done=True here (end of episode)\u001b[39;00m\n\u001b[0;32m--> 249\u001b[0m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_agents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m    \u001b[49m\u001b[43mold_obs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_observations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m    \u001b[49m\u001b[43mactions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfinal_actions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrewards\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrewards\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnew_obs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_observations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# or next obs in a multi-step setup\u001b[39;49;00m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdone\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    255\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRewards: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrewards\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/VS Code/GitHub Repositories/RL_Signaling/environment.py:363\u001b[0m, in \u001b[0;36mTempNetMultiAgentEnv.update_agents\u001b[0;34m(self, old_obs, actions, rewards, new_obs, done)\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_agents):\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magents[i]\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m    357\u001b[0m         state\u001b[38;5;241m=\u001b[39mold_obs[i],\n\u001b[1;32m    358\u001b[0m         action\u001b[38;5;241m=\u001b[39mactions[i],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m         done\u001b[38;5;241m=\u001b[39mdone\n\u001b[1;32m    362\u001b[0m     )\n\u001b[0;32m--> 363\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistories[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msignal_history\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignal_usage\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    364\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistories[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maction_history\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(copy\u001b[38;5;241m.\u001b[39mdeepcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_usage[i]))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/copy.py:146\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m copier \u001b[38;5;241m=\u001b[39m _deepcopy_dispatch\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/copy.py:231\u001b[0m, in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    229\u001b[0m memo[\u001b[38;5;28mid\u001b[39m(x)] \u001b[38;5;241m=\u001b[39m y\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 231\u001b[0m     y[\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m] \u001b[38;5;241m=\u001b[39m deepcopy(value, memo)\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/copy.py:146\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m copier \u001b[38;5;241m=\u001b[39m _deepcopy_dispatch\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/copy.py:220\u001b[0m, in \u001b[0;36m_deepcopy_tuple\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(x, y):\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m j:\n\u001b[0;32m--> 220\u001b[0m         y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def parameter_search(\n",
    "    param_grid,\n",
    "    n_trials=3,\n",
    "    n_episodes=10000,\n",
    "    base_seed=42\n",
    "):\n",
    "    results = []\n",
    "\n",
    "    # Generate all combinations of hyperparameters\n",
    "    keys = list(param_grid.keys())\n",
    "    combos = list(itertools.product(*[param_grid[k] for k in keys]))\n",
    "\n",
    "    for combo in combos:\n",
    "        params = dict(zip(keys, combo))\n",
    "        avg_rewards = []\n",
    "        avg_final_nmi = []\n",
    "\n",
    "        for trial in range(n_trials):\n",
    "            seed = base_seed + trial\n",
    "            np.random.seed(seed)\n",
    "\n",
    "            # Setup graph\n",
    "            G = nx.DiGraph()\n",
    "            G.add_nodes_from([0, 1])\n",
    "            G.add_edges_from([(0, 1), (1, 0)])\n",
    "\n",
    "            # Setup game and env\n",
    "            n_agents = 2\n",
    "            n_features = 2\n",
    "            n_signaling_actions = 2\n",
    "            n_final_actions = 4\n",
    "            agents_observed_variables = {0: [0], 1: [1]}\n",
    "            game = {i: create_random_canonical_game(n_features, n_final_actions, n=1, m=0)\n",
    "                    for i in range(n_agents)}\n",
    "\n",
    "            env = TempNetMultiAgentEnv(\n",
    "                n_agents=n_agents,\n",
    "                n_features=n_features,\n",
    "                n_signaling_actions=n_signaling_actions,\n",
    "                n_final_actions=n_final_actions,\n",
    "                full_information=False,\n",
    "                game_dicts=game,\n",
    "                observed_variables=agents_observed_variables,\n",
    "                agent_type=TDLearningAgent,\n",
    "                graph=G\n",
    "            )\n",
    "\n",
    "            # Override agents manually with hyperparameters\n",
    "            env.agents = [\n",
    "                TDLearningAgent(\n",
    "                    n_actions=env.max_actions,\n",
    "                    learning_rate=params['learning_rate'],\n",
    "                    exploration_rate=params['exploration_rate'],\n",
    "                    exploration_decay=params['exploration_decay'],\n",
    "                    min_exploration_rate=params['min_exploration_rate']\n",
    "                ) for _ in range(n_agents)\n",
    "            ]\n",
    "\n",
    "            _, rewards_history, signal_information_history, _, _ = temp_simulation_function(\n",
    "                n_agents=n_agents,\n",
    "                n_features=n_features,\n",
    "                n_signaling_actions=n_signaling_actions,\n",
    "                n_final_actions=n_final_actions,\n",
    "                n_episodes=n_episodes,\n",
    "                with_signals=True,\n",
    "                plot=False,\n",
    "                env=env,\n",
    "                verbose=False\n",
    "            )\n",
    "\n",
    "            # Measure average reward in last 10% of episodes\n",
    "            final_rewards = [\n",
    "                np.mean(rewards[-n_episodes // 10:]) for rewards in rewards_history\n",
    "            ]\n",
    "            avg_rewards.append(np.mean(final_rewards))\n",
    "\n",
    "            # Measure final normalized mutual information\n",
    "            final_nmi = [\n",
    "                np.mean(agent_nmi[-n_episodes // 10:]) if len(agent_nmi) >= n_episodes // 10 else 0.0\n",
    "                for agent_nmi in signal_information_history\n",
    "            ]\n",
    "            avg_final_nmi.append(np.mean(final_nmi))\n",
    "\n",
    "        result = {\n",
    "            'params': params,\n",
    "            'mean_reward': np.mean(avg_rewards),\n",
    "            'std_reward': np.std(avg_rewards),\n",
    "            'mean_final_nmi': np.mean(avg_final_nmi)\n",
    "        }\n",
    "        results.append(result)\n",
    "        print(f\"Tested: {params} => Mean Final Reward: {result['mean_reward']:.3f}, Mean Final NMI: {result['mean_final_nmi']:.3f}\")\n",
    "\n",
    "    return sorted(results, key=lambda r: -r['mean_reward'])\n",
    "\n",
    "\n",
    "# Example usage\n",
    "param_grid = {\n",
    "    'learning_rate': [0.1, 0.05, 0.01],\n",
    "    'exploration_rate': [1.0, 0.5],\n",
    "    'exploration_decay': [0.995, 0.90, 0.9],\n",
    "    'min_exploration_rate': [0.001, 0.0001]\n",
    "}\n",
    "\n",
    "search_results = parameter_search(param_grid, n_trials=100, n_episodes=5000)\n",
    "\n",
    "for r in search_results:\n",
    "    print(r)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
