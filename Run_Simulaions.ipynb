{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["7ohHXU7CJcna","wHWNzewd9D7t","XH_MrAiF-G2v","_vJ62NkB-G2w","grczen2I-G2x"],"machine_shape":"hm","authorship_tag":"ABX9TyOFbS6x6kdGY3Alj4U3isIm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Setup"],"metadata":{"id":"7ohHXU7CJcna"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-esY6VTUsINo","executionInfo":{"status":"ok","timestamp":1744779703047,"user_tz":-120,"elapsed":3928,"user":{"displayName":"Ignacio Ojea","userId":"11425136657122854785"}},"outputId":"8372761f-99f5-4452-cbab-793b725244c5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'RL_Signaling'...\n","remote: Enumerating objects: 639, done.\u001b[K\n","remote: Counting objects: 100% (31/31), done.\u001b[K\n","remote: Compressing objects: 100% (25/25), done.\u001b[K\n","remote: Total 639 (delta 10), reused 23 (delta 6), pack-reused 608 (from 1)\u001b[K\n","Receiving objects: 100% (639/639), 35.67 MiB | 20.78 MiB/s, done.\n","Resolving deltas: 100% (380/380), done.\n"]}],"source":["!git clone https://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@github.com/IgnacioOQ/RL_Signaling"]},{"cell_type":"code","source":["%cd RL_Signaling"],"metadata":{"id":"pGd6Q9RCroNo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1744779703091,"user_tz":-120,"elapsed":40,"user":{"displayName":"Ignacio Ojea","userId":"11425136657122854785"}},"outputId":"85abb26a-5c4b-47b1-8344-40eae9ea6f28"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/RL_Signaling\n"]}]},{"cell_type":"code","source":["from imports import *\n","from utils import *\n","from agents import UrnAgent, QLearningAgent, TDLearningAgent\n","from environment import NetMultiAgentEnv, TempNetMultiAgentEnv\n","from simulation_function import simulation_function, temp_simulation_function\n","\n","from joblib import Parallel, delayed, cpu_count\n","import multiprocessing\n","from datetime import datetime\n"],"metadata":{"id":"SXZgeb2UIoFV","executionInfo":{"status":"ok","timestamp":1744779705234,"user_tz":-120,"elapsed":2142,"user":{"displayName":"Ignacio Ojea","userId":"11425136657122854785"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# Decide where to put the files and do the working\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","dump_path = '/content/drive/My Drive/Colab Projects/Python ABMs/Communication/'\n","print(\"Current Directory:\", dump_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QHj4B6z9GZjJ","executionInfo":{"status":"ok","timestamp":1744779735107,"user_tz":-120,"elapsed":29870,"user":{"displayName":"Ignacio Ojea","userId":"11425136657122854785"}},"outputId":"66a76bf5-ecda-4740-9085-620bba1e7ec3"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Current Directory: /content/drive/My Drive/Colab Projects/Python ABMs/Communication/\n"]}]},{"cell_type":"markdown","source":["# Cannonical Model\n","\n","- World States: Two binary variables X, Y\n","- agents_observed_variables = {0:[0],1:[1]}\n","- Random Cannonical Games\n","- n_features = 2\n","- n_signaling_actions = 2\n","- n_final_actions = 4"],"metadata":{"id":"BmyFzfl78vnt"}},{"cell_type":"code","source":["add_data = False\n","n_iterations = 10"],"metadata":{"id":"0J5fd0wU_YgP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Urn Agent"],"metadata":{"id":"wHWNzewd9D7t"}},{"cell_type":"code","source":["simulate=False\n","if simulate:\n","  add_data = False\n","  n_iterations = 10000\n","\n","  # Report available CPU cores\n","  n_cores = cpu_count()\n","  print(f\"Using all available CPU cores: {n_cores}\")\n","\n","  # Define column names\n","  column_names = [\n","      'iteration', 'n_signaling_actions', 'n_final_actions', 'full_information', 'with_signals',\n","      'Agent_0_Initial_NMI', 'Agent_0_NMI', 'Agent_0_avg_reward', 'Agent_0_final_reward',\n","      'Agent_1_Initial_NMI', 'Agent_1_NMI', 'Agent_1_avg_reward', 'Agent_1_final_reward'\n","  ]\n","\n","  # Simulation parameters\n","  n_episodes = 10000\n","  n_agents = 2\n","  n_features = 2\n","  n_signaling_actions = 2\n","  n_final_actions = 4\n","\n","  def run_single_case(iteration, full_info, with_signals, game_dicts, obs_vars, graph):\n","      env = NetMultiAgentEnv(\n","          n_agents=n_agents,\n","          n_features=n_features,\n","          n_signaling_actions=n_signaling_actions,\n","          n_final_actions=n_final_actions,\n","          full_information=full_info,\n","          game_dicts=game_dicts,\n","          observed_variables=obs_vars,\n","          agent_type=UrnAgent,\n","          initialize=False,\n","          graph=graph\n","      )\n","\n","      results = [iteration, n_signaling_actions, n_final_actions, full_info, with_signals]\n","\n","      signal_usage, rewards_history, signal_information_history, _, _ = simulation_function(\n","          n_agents=n_agents,\n","          n_features=n_features,\n","          n_signaling_actions=n_signaling_actions,\n","          n_final_actions=n_final_actions,\n","          n_episodes=n_episodes,\n","          with_signals=with_signals,\n","          plot=False,\n","          env=env,\n","          verbose=False\n","      )\n","\n","      for agent_id in range(n_agents):\n","          info_hist = signal_information_history[agent_id]\n","          reward_hist = rewards_history[agent_id]\n","          results.extend([\n","              np.mean(info_hist[:10]),\n","              np.mean(info_hist[-100:]),\n","              np.mean(reward_hist),\n","              np.mean(reward_hist[-100:])\n","          ])\n","\n","      return results\n","\n","  def run_all_cases_for_iteration(iteration):\n","      game_dicts = {i: create_random_canonical_game(n_features, n_final_actions, n=1, m=0) for i in range(n_agents)}\n","      obs_vars = {0: [0], 1: [1]}\n","      G = nx.DiGraph()\n","      G.add_edges_from([(0, 1), (1, 0)])\n","\n","      cases = [(False, False), (False, True), (True, False), (True, True)]\n","      return [run_single_case(iteration, fi, ws, game_dicts, obs_vars, G) for fi, ws in cases]\n","\n","  # Run in parallel\n","  all_results = Parallel(n_jobs=n_cores)(\n","      delayed(run_all_cases_for_iteration)(i) for i in tqdm(range(n_iterations), desc=\"Running UrnAgent simulations\")\n","  )\n","\n","  # Flatten results and create DataFrame\n","  flat_results = [row for group in all_results for row in group]\n","  results_df = pd.DataFrame(flat_results, columns=column_names)\n","\n","  # Append or save\n","  output_file = dump_path+'urnagent_results_cannonical.csv'\n","  if add_data:\n","      old_results_df = pd.read_csv(output_file)\n","      total_results_df = pd.concat([old_results_df, results_df], ignore_index=True)\n","  else:\n","      total_results_df = results_df\n","\n","  total_results_df.to_csv(output_file, index=False)\n","  print(f\"Total rows in saved file: {len(total_results_df)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dIu7XxGC9FPr","executionInfo":{"status":"ok","timestamp":1744731963162,"user_tz":-120,"elapsed":30334979,"user":{"displayName":"Ignacio Ojea","userId":"11425136657122854785"}},"outputId":"423a10e9-ec04-40db-f51f-6f74738b9ef0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using all available CPU cores: 8\n"]},{"output_type":"stream","name":"stderr","text":["Running UrnAgent simulations: 100%|██████████| 10000/10000 [8:24:54<00:00,  3.03s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Total rows in saved file: 40000\n"]}]},{"cell_type":"markdown","source":["## Q-Learning"],"metadata":{"id":"grw11cp4JS2z"}},{"cell_type":"code","source":["simulate=False\n","if simulate:\n","  add_data = False\n","  n_iterations = 10000\n","\n","  # Print number of available CPU cores\n","  n_cores = cpu_count()\n","  print(f\"Using all available CPU cores: {n_cores}\")\n","\n","  # Define column names\n","  column_names = [\n","      'iteration', 'n_signaling_actions', 'n_final_actions', 'full_information', 'with_signals',\n","      'Agent_0_Initial_NMI', 'Agent_0_NMI', 'Agent_0_avg_reward', 'Agent_0_final_reward',\n","      'Agent_1_Initial_NMI', 'Agent_1_NMI', 'Agent_1_avg_reward', 'Agent_1_final_reward'\n","  ]\n","\n","  n_episodes = 10000\n","  n_agents = 2\n","  n_features = 2\n","  n_signaling_actions = 2\n","  n_final_actions = 4\n","\n","  def run_single_case(iteration, full_info, with_signals, game_dicts, obs_vars, graph):\n","      env = NetMultiAgentEnv(\n","          n_agents=n_agents,\n","          n_features=n_features,\n","          n_signaling_actions=n_signaling_actions,\n","          n_final_actions=n_final_actions,\n","          full_information=full_info,\n","          game_dicts=game_dicts,\n","          observed_variables=obs_vars,\n","          agent_type=QLearningAgent,\n","          initialize=False,\n","          graph=graph\n","      )\n","\n","      env.agents = [\n","          QLearningAgent(\n","              n_signaling_actions=n_signaling_actions,\n","              n_final_actions=n_final_actions,\n","              exploration_rate=0.9976461429984532,\n","              exploration_decay=0.9805828477324336,\n","              min_exploration_rate=0.002583111144750034,\n","              initialize=False\n","          ) for _ in range(n_agents)\n","      ]\n","\n","      results = [iteration, n_signaling_actions, n_final_actions, full_info, with_signals]\n","\n","      signal_usage, rewards_history, signal_information_history, *_ = simulation_function(\n","          n_agents=n_agents,\n","          n_features=n_features,\n","          n_signaling_actions=n_signaling_actions,\n","          n_final_actions=n_final_actions,\n","          n_episodes=n_episodes,\n","          with_signals=with_signals,\n","          plot=False,\n","          env=env,\n","          verbose=False\n","      )\n","\n","      for agent_id in range(n_agents):\n","          info_hist = signal_information_history[agent_id]\n","          reward_hist = rewards_history[agent_id]\n","          results.extend([\n","              np.mean(info_hist[:10]),\n","              np.mean(info_hist[-100:]),\n","              np.mean(reward_hist),\n","              np.mean(reward_hist[-100:])\n","          ])\n","\n","      return results\n","\n","  def run_all_cases_for_iteration(iteration):\n","      # Prepare shared data for all 4 cases\n","      game_dicts = {i: create_random_canonical_game(n_features, n_final_actions) for i in range(n_agents)}\n","      obs_vars = {0: [0], 1: [1]}\n","      G = nx.DiGraph()\n","      G.add_edges_from([(0, 1), (1, 0)])\n","\n","      cases = [(False, False), (False, True), (True, False), (True, True)]\n","      return [run_single_case(iteration, fi, ws, game_dicts, obs_vars, G) for fi, ws in cases]\n","\n","  # Run simulations in parallel using all available cores\n","  all_results = Parallel(n_jobs=n_cores)(\n","      delayed(run_all_cases_for_iteration)(i) for i in tqdm(range(n_iterations), desc=\"Running in parallel\")\n","  )\n","\n","  # Flatten the list of lists\n","  flat_results = [row for group in all_results for row in group]\n","  results_df = pd.DataFrame(flat_results, columns=column_names)\n","\n","  # Append or save\n","  output_file = dump_path+'qlearning_results_cannonical.csv'\n","  if add_data:\n","      old_results_df = pd.read_csv(output_file)\n","      total_results_df = pd.concat([old_results_df, results_df], ignore_index=True)\n","  else:\n","      total_results_df = results_df\n","\n","  total_results_df.to_csv(output_file, index=False)\n","  print(f\"Total rows: {len(total_results_df)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6d_bpnFR7SNS","executionInfo":{"status":"ok","timestamp":1744753144962,"user_tz":-120,"elapsed":20403095,"user":{"displayName":"Ignacio Ojea","userId":"11425136657122854785"}},"outputId":"b6d2174a-178c-4ae9-8dff-a85fb47ff406"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using all available CPU cores: 8\n"]},{"output_type":"stream","name":"stderr","text":["Running in parallel: 100%|██████████| 10000/10000 [5:39:38<00:00,  2.04s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Total rows: 40000\n"]}]},{"cell_type":"markdown","source":["## TD Agent"],"metadata":{"id":"xLOuDhY08iXf"}},{"cell_type":"code","source":["n_iterations = 100\n","# Report CPU info\n","n_cores = cpu_count()\n","print(f\"Using all available CPU cores: {n_cores}\")\n","\n","# Define output columns\n","column_names = [\n","    'iteration', 'n_signaling_actions', 'n_final_actions', 'full_information', 'with_signals',\n","    'Agent_0_Initial_NMI', 'Agent_0_NMI', 'Agent_0_avg_reward', 'Agent_0_final_reward',\n","    'Agent_1_Initial_NMI', 'Agent_1_NMI', 'Agent_1_avg_reward', 'Agent_1_final_reward'\n","]\n","\n","# Global params\n","n_episodes = 10000\n","n_agents = 2\n","n_features = 2\n","n_signaling_actions = 2\n","n_final_actions = 4\n","\n","def run_single_case(iteration, full_info, with_signals, game_dicts, obs_vars, graph):\n","    env = TempNetMultiAgentEnv(\n","        n_agents=n_agents,\n","        n_features=n_features,\n","        n_signaling_actions=n_signaling_actions,\n","        n_final_actions=n_final_actions,\n","        learning_rate=0.1,\n","        exploration_rate=1.0,\n","        exploration_decay=0.995,\n","        min_exploration_rate=0.001,\n","        full_information=full_info,\n","        game_dicts=game_dicts,\n","        observed_variables=obs_vars,\n","        agent_type=TDLearningAgent,\n","        graph=graph\n","    )\n","\n","    env.agents = [\n","            TDLearningAgent(\n","                n_actions=env.max_actions,\n","                learning_rate=0.1,  # Fixed learning rate\n","                exploration_rate=0.6733441159316643,\n","                exploration_decay=0.9865577543877726,\n","                min_exploration_rate=0.0013741277073265228,\n","                gamma=0.9729821735549989\n","            ) for _ in range(n_agents)\n","        ]\n","\n","    results = [iteration, n_signaling_actions, n_final_actions, full_info, with_signals]\n","\n","    signal_usage, rewards_history, signal_information_history, _, _ = temp_simulation_function(\n","        n_agents=n_agents,\n","        n_features=n_features,\n","        n_signaling_actions=n_signaling_actions,\n","        n_final_actions=n_final_actions,\n","        n_episodes=n_episodes,\n","        with_signals=with_signals,\n","        plot=False,\n","        env=env,\n","        verbose=False\n","    )\n","\n","    for agent_id in range(n_agents):\n","        info_hist = signal_information_history[agent_id]\n","        reward_hist = rewards_history[agent_id]\n","        results.extend([\n","            np.mean(info_hist[:10]),\n","            np.mean(info_hist[-100:]),\n","            np.mean(reward_hist),\n","            np.mean(reward_hist[-100:])\n","        ])\n","\n","    return results\n","\n","def run_all_cases_for_iteration(iteration):\n","    # Create fresh game and graph for this iteration\n","    game_dicts = {i: create_random_canonical_game(n_features, n_final_actions, n=1, m=0) for i in range(n_agents)}\n","    obs_vars = {0: [0], 1: [1]}\n","    G = nx.DiGraph()\n","    G.add_edges_from([(0, 1), (1, 0)])\n","\n","    cases = [(False, False), (False, True), (True, False), (True, True)]\n","    return [run_single_case(iteration, fi, ws, game_dicts, obs_vars, G) for fi, ws in cases]\n","\n","# Run all in parallel\n","all_results = Parallel(n_jobs=n_cores)(\n","    delayed(run_all_cases_for_iteration)(i) for i in tqdm(range(n_iterations), desc=\"Running parallel simulations\")\n",")\n","\n","# Flatten results\n","flat_results = [row for group in all_results for row in group]\n","results_df = pd.DataFrame(flat_results, columns=column_names)\n","\n","# Save or append to file\n","output_file = dump_path+'td_learning_results_cannonical.csv'\n","if add_data:\n","    old_results_df = pd.read_csv(output_file)\n","    total_results_df = pd.concat([old_results_df, results_df], ignore_index=True)\n","else:\n","    total_results_df = results_df\n","\n","total_results_df.to_csv(output_file, index=False)\n","print(f\"Total rows in saved file: {len(total_results_df)}\")"],"metadata":{"id":"drUE71FK8jSP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# More Complex Model\n","\n","- World States: Three binary variables X, Y, Z\n","- agents_observed_variables = {0:[0,1],1:[1,2]}\n","- n_features = 3 #parameters['n_features']\n","- n_signaling_actions = 4 #parameters['n_signaling_actions']\n","- n_final_actions = 8 #parameters['n_final_actions']\n","- Random Games (possibly non-cannonical)"],"metadata":{"id":"ntQk17Vm9vnu"}},{"cell_type":"markdown","source":["## Urn Agent"],"metadata":{"id":"XH_MrAiF-G2v"}},{"cell_type":"code","source":["simulate=True\n","if simulate:\n","  add_data = False\n","  n_iterations = 10000\n","\n","  # Report available CPU cores\n","  n_cores = cpu_count()\n","  print(f\"Using all available CPU cores: {n_cores}\")\n","\n","  # Define column names\n","  column_names = [\n","      'iteration', 'n_signaling_actions', 'n_final_actions', 'full_information', 'with_signals',\n","      'Agent_0_Initial_NMI', 'Agent_0_NMI', 'Agent_0_avg_reward', 'Agent_0_final_reward',\n","      'Agent_1_Initial_NMI', 'Agent_1_NMI', 'Agent_1_avg_reward', 'Agent_1_final_reward'\n","  ]\n","\n","  # Simulation parameters\n","  n_episodes = 10000\n","  n_agents = 2\n","  n_features = 3\n","  n_signaling_actions = 4\n","  n_final_actions = 8\n","\n","  def run_single_case(iteration, full_info, with_signals, game_dicts, obs_vars, graph):\n","      env = NetMultiAgentEnv(\n","          n_agents=n_agents,\n","          n_features=n_features,\n","          n_signaling_actions=n_signaling_actions,\n","          n_final_actions=n_final_actions,\n","          full_information=full_info,\n","          game_dicts=game_dicts,\n","          observed_variables=obs_vars,\n","          agent_type=UrnAgent,\n","          initialize=False,\n","          graph=graph\n","      )\n","\n","      results = [iteration, n_signaling_actions, n_final_actions, full_info, with_signals]\n","\n","      signal_usage, rewards_history, signal_information_history, _, _ = simulation_function(\n","          n_agents=n_agents,\n","          n_features=n_features,\n","          n_signaling_actions=n_signaling_actions,\n","          n_final_actions=n_final_actions,\n","          n_episodes=n_episodes,\n","          with_signals=with_signals,\n","          plot=False,\n","          env=env,\n","          verbose=False\n","      )\n","\n","      for agent_id in range(n_agents):\n","          info_hist = signal_information_history[agent_id]\n","          reward_hist = rewards_history[agent_id]\n","          results.extend([\n","              np.mean(info_hist[:10]),\n","              np.mean(info_hist[-100:]),\n","              np.mean(reward_hist),\n","              np.mean(reward_hist[-100:])\n","          ])\n","\n","      return results\n","\n","  def run_all_cases_for_iteration(iteration):\n","      game_dicts = {i: create_random_canonical_game(n_features, n_final_actions, n=1, m=0) for i in range(n_agents)}\n","      obs_vars = {0:[0,1],1:[1,2]}\n","      G = nx.DiGraph()\n","      G.add_edges_from([(0, 1), (1, 0)])\n","\n","      cases = [(False, False), (False, True), (True, False), (True, True)]\n","      return [run_single_case(iteration, fi, ws, game_dicts, obs_vars, G) for fi, ws in cases]\n","\n","  # Run in parallel\n","  all_results = Parallel(n_jobs=n_cores)(\n","      delayed(run_all_cases_for_iteration)(i) for i in tqdm(range(n_iterations), desc=\"Running UrnAgent simulations\")\n","  )\n","\n","  # Flatten results and create DataFrame\n","  flat_results = [row for group in all_results for row in group]\n","  results_df = pd.DataFrame(flat_results, columns=column_names)\n","\n","  # Append or save\n","  output_file = dump_path+'urnagent_results_complex.csv'\n","  if add_data:\n","      old_results_df = pd.read_csv(output_file)\n","      total_results_df = pd.concat([old_results_df, results_df], ignore_index=True)\n","  else:\n","      total_results_df = results_df\n","\n","  total_results_df.to_csv(output_file, index=False)\n","  print(f\"Total rows in saved file: {len(total_results_df)}\")"],"metadata":{"id":"ViRgyORJ-G2w","colab":{"base_uri":"https://localhost:8080/"},"outputId":"48a1ec48-b06b-4270-ddfa-7dcfa251a73b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using all available CPU cores: 8\n"]},{"output_type":"stream","name":"stderr","text":["Running UrnAgent simulations:   2%|▏         | 152/10000 [14:17<15:59:21,  5.84s/it]"]}]},{"cell_type":"markdown","source":["## Q-Learning"],"metadata":{"id":"_vJ62NkB-G2w"}},{"cell_type":"code","source":["# n_iterations = 10\n","# Print number of available CPU cores\n","n_cores = cpu_count()\n","print(f\"Using all available CPU cores: {n_cores}\")\n","\n","# Define column names\n","column_names = [\n","    'iteration', 'n_signaling_actions', 'n_final_actions', 'full_information', 'with_signals',\n","    'Agent_0_Initial_NMI', 'Agent_0_NMI', 'Agent_0_avg_reward', 'Agent_0_final_reward',\n","    'Agent_1_Initial_NMI', 'Agent_1_NMI', 'Agent_1_avg_reward', 'Agent_1_final_reward'\n","]\n","\n","n_episodes = 10000\n","n_agents = 2\n","n_features = 3\n","n_signaling_actions = 4\n","n_final_actions = 8\n","\n","def run_single_case(iteration, full_info, with_signals, game_dicts, obs_vars, graph):\n","    env = NetMultiAgentEnv(\n","        n_agents=n_agents,\n","        n_features=n_features,\n","        n_signaling_actions=n_signaling_actions,\n","        n_final_actions=n_final_actions,\n","        full_information=full_info,\n","        game_dicts=game_dicts,\n","        observed_variables=obs_vars,\n","        agent_type=QLearningAgent,\n","        initialize=False,\n","        graph=graph\n","    )\n","\n","    env.agents = [\n","            QLearningAgent(\n","                n_signaling_actions=n_signaling_actions,\n","                n_final_actions=n_final_actions,\n","                exploration_rate=1,\n","                exploration_decay=0.995,\n","                min_exploration_rate=0.0001,\n","                initialize=False\n","            ) for _ in range(n_agents)\n","        ]\n","\n","    results = [iteration, n_signaling_actions, n_final_actions, full_info, with_signals]\n","\n","    signal_usage, rewards_history, signal_information_history, *_ = simulation_function(\n","        n_agents=n_agents,\n","        n_features=n_features,\n","        n_signaling_actions=n_signaling_actions,\n","        n_final_actions=n_final_actions,\n","        n_episodes=n_episodes,\n","        with_signals=with_signals,\n","        plot=False,\n","        env=env,\n","        verbose=False\n","    )\n","\n","    for agent_id in range(n_agents):\n","        info_hist = signal_information_history[agent_id]\n","        reward_hist = rewards_history[agent_id]\n","        results.extend([\n","            np.mean(info_hist[:10]),\n","            np.mean(info_hist[-100:]),\n","            np.mean(reward_hist),\n","            np.mean(reward_hist[-100:])\n","        ])\n","\n","    return results\n","\n","def run_all_cases_for_iteration(iteration):\n","    # Prepare shared data for all 4 cases\n","    game_dicts = {i: create_random_canonical_game(n_features, n_final_actions) for i in range(n_agents)}\n","    obs_vars = {0:[0,1],1:[1,2]}\n","    G = nx.DiGraph()\n","    G.add_edges_from([(0, 1), (1, 0)])\n","\n","    cases = [(False, False), (False, True), (True, False), (True, True)]\n","    return [run_single_case(iteration, fi, ws, game_dicts, obs_vars, G) for fi, ws in cases]\n","\n","# Run simulations in parallel using all available cores\n","all_results = Parallel(n_jobs=n_cores)(\n","    delayed(run_all_cases_for_iteration)(i) for i in tqdm(range(n_iterations), desc=\"Running in parallel\")\n",")\n","\n","# Flatten the list of lists\n","flat_results = [row for group in all_results for row in group]\n","results_df = pd.DataFrame(flat_results, columns=column_names)\n","\n","# Append or save\n","output_file = dump_path+'qlearning_results_complex.csv'\n","if add_data:\n","    old_results_df = pd.read_csv(output_file)\n","    total_results_df = pd.concat([old_results_df, results_df], ignore_index=True)\n","else:\n","    total_results_df = results_df\n","\n","total_results_df.to_csv(output_file, index=False)\n","print(f\"Total rows: {len(total_results_df)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1744631182970,"user_tz":-120,"elapsed":82293,"user":{"displayName":"Ignacio Ojea","userId":"11425136657122854785"}},"outputId":"83a6d788-4c04-4039-9a90-dbdf4b0b054b","id":"cDETgMDw-G2x"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using all available CPU cores: 8\n"]},{"output_type":"stream","name":"stderr","text":["Running in parallel: 100%|██████████| 10/10 [00:00<00:00, 7113.81it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Total rows: 40\n"]}]},{"cell_type":"markdown","source":["## TD Agent"],"metadata":{"id":"grczen2I-G2x"}},{"cell_type":"code","source":["# n_iterations = 10\n","# Report CPU info\n","n_cores = cpu_count()\n","print(f\"Using all available CPU cores: {n_cores}\")\n","\n","# Define output columns\n","column_names = [\n","    'iteration', 'n_signaling_actions', 'n_final_actions', 'full_information', 'with_signals',\n","    'Agent_0_Initial_NMI', 'Agent_0_NMI', 'Agent_0_avg_reward', 'Agent_0_final_reward',\n","    'Agent_1_Initial_NMI', 'Agent_1_NMI', 'Agent_1_avg_reward', 'Agent_1_final_reward'\n","]\n","\n","# Global params\n","n_episodes = 10000\n","n_agents = 2\n","n_features = 3\n","n_signaling_actions = 4\n","n_final_actions = 8\n","\n","def run_single_case(iteration, full_info, with_signals, game_dicts, obs_vars, graph):\n","    env = TempNetMultiAgentEnv(\n","        n_agents=n_agents,\n","        n_features=n_features,\n","        n_signaling_actions=n_signaling_actions,\n","        n_final_actions=n_final_actions,\n","        learning_rate=0.1,\n","        exploration_rate=1.0,\n","        exploration_decay=0.995,\n","        min_exploration_rate=0.001,\n","        full_information=full_info,\n","        game_dicts=game_dicts,\n","        observed_variables=obs_vars,\n","        agent_type=TDLearningAgent,\n","        graph=graph\n","    )\n","\n","    env.agents = [\n","            TDLearningAgent(\n","                n_actions=env.max_actions,\n","                learning_rate=0.1,  # Fixed learning rate\n","                exploration_rate=1,\n","                exploration_decay=0.995,\n","                min_exploration_rate=0.0001\n","            ) for _ in range(n_agents)\n","        ]\n","\n","    results = [iteration, n_signaling_actions, n_final_actions, full_info, with_signals]\n","\n","    signal_usage, rewards_history, signal_information_history, _, _ = temp_simulation_function(\n","        n_agents=n_agents,\n","        n_features=n_features,\n","        n_signaling_actions=n_signaling_actions,\n","        n_final_actions=n_final_actions,\n","        n_episodes=n_episodes,\n","        with_signals=with_signals,\n","        plot=False,\n","        env=env,\n","        verbose=False\n","    )\n","\n","    for agent_id in range(n_agents):\n","        info_hist = signal_information_history[agent_id]\n","        reward_hist = rewards_history[agent_id]\n","        results.extend([\n","            np.mean(info_hist[:10]),\n","            np.mean(info_hist[-100:]),\n","            np.mean(reward_hist),\n","            np.mean(reward_hist[-100:])\n","        ])\n","\n","    return results\n","\n","def run_all_cases_for_iteration(iteration):\n","    # Create fresh game and graph for this iteration\n","    game_dicts = {i: create_random_canonical_game(n_features, n_final_actions, n=1, m=0) for i in range(n_agents)}\n","    obs_vars = {0:[0,1],1:[1,2]}\n","    G = nx.DiGraph()\n","    G.add_edges_from([(0, 1), (1, 0)])\n","\n","    cases = [(False, False), (False, True), (True, False), (True, True)]\n","    return [run_single_case(iteration, fi, ws, game_dicts, obs_vars, G) for fi, ws in cases]\n","\n","# Run all in parallel\n","all_results = Parallel(n_jobs=n_cores)(\n","    delayed(run_all_cases_for_iteration)(i) for i in tqdm(range(n_iterations), desc=\"Running parallel simulations\")\n",")\n","\n","# Flatten results\n","flat_results = [row for group in all_results for row in group]\n","results_df = pd.DataFrame(flat_results, columns=column_names)\n","\n","# Save or append to file\n","output_file = dump_path+'td_learning_results_complex.csv'\n","if add_data:\n","    old_results_df = pd.read_csv(output_file)\n","    total_results_df = pd.concat([old_results_df, results_df], ignore_index=True)\n","else:\n","    total_results_df = results_df\n","\n","total_results_df.to_csv(output_file, index=False)\n","print(f\"Total rows in saved file: {len(total_results_df)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"811b52f5-feaf-4081-8556-58aab64da145","id":"NVFoEBoC-G2y","executionInfo":{"status":"ok","timestamp":1744631336663,"user_tz":-120,"elapsed":153699,"user":{"displayName":"Ignacio Ojea","userId":"11425136657122854785"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using all available CPU cores: 8\n"]},{"output_type":"stream","name":"stderr","text":["Running parallel simulations: 100%|██████████| 10/10 [00:00<00:00, 6440.88it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Total rows in saved file: 40\n"]}]}]}