{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "7ohHXU7CJcna",
        "wHWNzewd9D7t",
        "grw11cp4JS2z",
        "XH_MrAiF-G2v",
        "grczen2I-G2x"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "7ohHXU7CJcna"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-esY6VTUsINo",
        "outputId": "d0b3f13a-cfde-428a-fbfc-b017807e91ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'RL_Signaling'...\n",
            "remote: Enumerating objects: 697, done.\u001b[K\n",
            "remote: Counting objects: 100% (89/89), done.\u001b[K\n",
            "remote: Compressing objects: 100% (73/73), done.\u001b[K\n",
            "remote: Total 697 (delta 33), reused 69 (delta 16), pack-reused 608 (from 1)\u001b[K\n",
            "Receiving objects: 100% (697/697), 47.60 MiB | 38.32 MiB/s, done.\n",
            "Resolving deltas: 100% (403/403), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://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@github.com/IgnacioOQ/RL_Signaling"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd RL_Signaling"
      ],
      "metadata": {
        "id": "pGd6Q9RCroNo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b7622c9-ade2-4d0f-8d9c-6f247f74fe33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/RL_Signaling\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from imports import *\n",
        "from utils import *\n",
        "from agents import UrnAgent, QLearningAgent, TDLearningAgent\n",
        "from environment import NetMultiAgentEnv, TempNetMultiAgentEnv\n",
        "from simulation_function import simulation_function, temp_simulation_function\n",
        "\n",
        "from joblib import Parallel, delayed, cpu_count\n",
        "import multiprocessing\n",
        "from datetime import datetime\n"
      ],
      "metadata": {
        "id": "SXZgeb2UIoFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decide where to put the files and do the working\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "dump_path = '/content/drive/My Drive/Colab Projects/Python ABMs/Communication/Plots and Datasets/'\n",
        "print(\"Current Directory:\", dump_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHj4B6z9GZjJ",
        "outputId": "f090c4fb-5e9f-40b2-b849-b766fa84a679"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Current Directory: /content/drive/My Drive/Colab Projects/Python ABMs/Communication/Plots and Datasets/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cannonical Model\n",
        "\n",
        "- World States: Two binary variables X, Y\n",
        "- agents_observed_variables = {0:[0],1:[1]}\n",
        "- Random Cannonical Games\n",
        "- n_features = 2\n",
        "- n_signaling_actions = 2\n",
        "- n_final_actions = 4"
      ],
      "metadata": {
        "id": "BmyFzfl78vnt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "add_data = False\n",
        "n_iterations = 10"
      ],
      "metadata": {
        "id": "0J5fd0wU_YgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Urn Agent"
      ],
      "metadata": {
        "id": "wHWNzewd9D7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "simulate=False\n",
        "if simulate:\n",
        "  add_data = False\n",
        "  n_iterations = 10000\n",
        "\n",
        "  # Report available CPU cores\n",
        "  n_cores = cpu_count()\n",
        "  print(f\"Using all available CPU cores: {n_cores}\")\n",
        "\n",
        "  # Define column names\n",
        "  column_names = [\n",
        "      'iteration', 'n_signaling_actions', 'n_final_actions', 'full_information', 'with_signals',\n",
        "      'Agent_0_Initial_NMI', 'Agent_0_NMI', 'Agent_0_avg_reward', 'Agent_0_final_reward',\n",
        "      'Agent_1_Initial_NMI', 'Agent_1_NMI', 'Agent_1_avg_reward', 'Agent_1_final_reward'\n",
        "  ]\n",
        "\n",
        "  # Simulation parameters\n",
        "  n_episodes = 10000\n",
        "  n_agents = 2\n",
        "  n_features = 2\n",
        "  n_signaling_actions = 2\n",
        "  n_final_actions = 4\n",
        "\n",
        "  def run_single_case(iteration, full_info, with_signals, game_dicts, obs_vars, graph):\n",
        "      #set seeds\n",
        "      np.random.seed(iteration)\n",
        "      random.seed(iteration)\n",
        "      # continue\n",
        "\n",
        "      env = NetMultiAgentEnv(\n",
        "          n_agents=n_agents,\n",
        "          n_features=n_features,\n",
        "          n_signaling_actions=n_signaling_actions,\n",
        "          n_final_actions=n_final_actions,\n",
        "          full_information=full_info,\n",
        "          game_dicts=game_dicts,\n",
        "          observed_variables=obs_vars,\n",
        "          agent_type=UrnAgent,\n",
        "          initialize=False,\n",
        "          graph=graph\n",
        "      )\n",
        "\n",
        "      results = [iteration, n_signaling_actions, n_final_actions, full_info, with_signals]\n",
        "\n",
        "      signal_usage, rewards_history, signal_information_history, _, _ = simulation_function(\n",
        "          n_agents=n_agents,\n",
        "          n_features=n_features,\n",
        "          n_signaling_actions=n_signaling_actions,\n",
        "          n_final_actions=n_final_actions,\n",
        "          n_episodes=n_episodes,\n",
        "          with_signals=with_signals,\n",
        "          plot=False,\n",
        "          env=env,\n",
        "          verbose=False\n",
        "      )\n",
        "\n",
        "      for agent_id in range(n_agents):\n",
        "          info_hist = signal_information_history[agent_id]\n",
        "          reward_hist = rewards_history[agent_id]\n",
        "          results.extend([\n",
        "              np.mean(info_hist[:10]),\n",
        "              np.mean(info_hist[-100:]),\n",
        "              np.mean(reward_hist),\n",
        "              np.mean(reward_hist[-100:])\n",
        "          ])\n",
        "\n",
        "      return results\n",
        "\n",
        "  def run_all_cases_for_iteration(iteration):\n",
        "      # continue\n",
        "      game_dicts = {i: create_random_canonical_game(n_features, n_final_actions, n=1, m=0) for i in range(n_agents)}\n",
        "      obs_vars = {0: [0], 1: [1]}\n",
        "      G = nx.DiGraph()\n",
        "      G.add_edges_from([(0, 1), (1, 0)])\n",
        "\n",
        "      cases = [(False, False), (False, True), (True, False), (True, True)]\n",
        "      return [run_single_case(iteration, fi, ws, game_dicts, obs_vars, G) for fi, ws in cases]\n",
        "\n",
        "  # Run in parallel\n",
        "  all_results = Parallel(n_jobs=n_cores)(\n",
        "      delayed(run_all_cases_for_iteration)(i) for i in tqdm(range(n_iterations), desc=\"Running UrnAgent simulations\")\n",
        "  )\n",
        "\n",
        "  # Flatten results and create DataFrame\n",
        "  flat_results = [row for group in all_results for row in group]\n",
        "  results_df = pd.DataFrame(flat_results, columns=column_names)\n",
        "\n",
        "  # Append or save\n",
        "  output_file = dump_path+'urnagent_results_cannonical.csv'\n",
        "  if add_data:\n",
        "      old_results_df = pd.read_csv(output_file)\n",
        "      total_results_df = pd.concat([old_results_df, results_df], ignore_index=True)\n",
        "  else:\n",
        "      total_results_df = results_df\n",
        "\n",
        "  total_results_df.to_csv(output_file, index=False)\n",
        "  print(f\"Total rows in saved file: {len(total_results_df)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIu7XxGC9FPr",
        "outputId": "29bb9bdb-c2ac-4642-b003-2847fc2e83fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using all available CPU cores: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Running UrnAgent simulations: 100%|██████████| 10000/10000 [10:55:58<00:00,  3.94s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total rows in saved file: 40000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q-Learning"
      ],
      "metadata": {
        "id": "grw11cp4JS2z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "simulate=False\n",
        "if simulate:\n",
        "  add_data = True\n",
        "  n_iterations = 2000\n",
        "\n",
        "  # Print number of available CPU cores\n",
        "  n_cores = cpu_count()\n",
        "  print(f\"Using all available CPU cores: {n_cores}\")\n",
        "\n",
        "  # Define column names\n",
        "  column_names = [\n",
        "      'iteration', 'n_signaling_actions', 'n_final_actions', 'full_information', 'with_signals',\n",
        "      'Agent_0_Initial_NMI', 'Agent_0_NMI', 'Agent_0_avg_reward', 'Agent_0_final_reward',\n",
        "      'Agent_1_Initial_NMI', 'Agent_1_NMI', 'Agent_1_avg_reward', 'Agent_1_final_reward'\n",
        "  ]\n",
        "\n",
        "  n_episodes = 10000\n",
        "  n_agents = 2\n",
        "  n_features = 2\n",
        "  n_signaling_actions = 2\n",
        "  n_final_actions = 4\n",
        "\n",
        "  def run_single_case(iteration, full_info, with_signals, game_dicts, obs_vars, graph):\n",
        "          #set seeds\n",
        "      np.random.seed(iteration)\n",
        "      random.seed(iteration)\n",
        "      # continue\n",
        "\n",
        "      env = NetMultiAgentEnv(\n",
        "          n_agents=n_agents,\n",
        "          n_features=n_features,\n",
        "          n_signaling_actions=n_signaling_actions,\n",
        "          n_final_actions=n_final_actions,\n",
        "          full_information=full_info,\n",
        "          game_dicts=game_dicts,\n",
        "          observed_variables=obs_vars,\n",
        "          agent_type=QLearningAgent,\n",
        "          initialize=False,\n",
        "          graph=graph\n",
        "      )\n",
        "\n",
        "      env.agents = [\n",
        "          QLearningAgent(\n",
        "              n_signaling_actions=n_signaling_actions,\n",
        "              n_final_actions=n_final_actions,\n",
        "              exploration_rate=0.9652628633727897,\n",
        "              exploration_decay=0.9998122815486062,\n",
        "              min_exploration_rate=1e-10,\n",
        "              choice = 'ucb'\n",
        "          ) for _ in range(n_agents)\n",
        "      ]\n",
        "\n",
        "      results = [iteration, n_signaling_actions, n_final_actions, full_info, with_signals]\n",
        "\n",
        "      signal_usage, rewards_history, signal_information_history, *_ = simulation_function(\n",
        "          n_agents=n_agents,\n",
        "          n_features=n_features,\n",
        "          n_signaling_actions=n_signaling_actions,\n",
        "          n_final_actions=n_final_actions,\n",
        "          n_episodes=n_episodes,\n",
        "          with_signals=with_signals,\n",
        "          plot=False,\n",
        "          env=env,\n",
        "          verbose=False\n",
        "      )\n",
        "\n",
        "      for agent_id in range(n_agents):\n",
        "          info_hist = signal_information_history[agent_id]\n",
        "          reward_hist = rewards_history[agent_id]\n",
        "          results.extend([\n",
        "              np.mean(info_hist[:10]),\n",
        "              np.mean(info_hist[-100:]),\n",
        "              np.mean(reward_hist),\n",
        "              np.mean(reward_hist[-100:])\n",
        "          ])\n",
        "\n",
        "      return results\n",
        "\n",
        "  def run_all_cases_for_iteration(iteration):\n",
        "      # Prepare shared data for all 4 cases\n",
        "      game_dicts = {i: create_random_canonical_game(n_features, n_final_actions) for i in range(n_agents)}\n",
        "      obs_vars = {0: [0], 1: [1]}\n",
        "      G = nx.DiGraph()\n",
        "      G.add_edges_from([(0, 1), (1, 0)])\n",
        "\n",
        "      cases = [(False, False), (False, True), (True, False), (True, True)]\n",
        "      return [run_single_case(iteration, fi, ws, game_dicts, obs_vars, G) for fi, ws in cases]\n",
        "\n",
        "  # Run simulations in parallel using all available cores\n",
        "  all_results = Parallel(n_jobs=n_cores)(\n",
        "      delayed(run_all_cases_for_iteration)(i) for i in tqdm(range(n_iterations), desc=\"Running in parallel\")\n",
        "  )\n",
        "\n",
        "  # Flatten the list of lists\n",
        "  flat_results = [row for group in all_results for row in group]\n",
        "  results_df = pd.DataFrame(flat_results, columns=column_names)\n",
        "\n",
        "  # Append or save\n",
        "  output_file = dump_path+'qlearning_results_cannonical.csv'\n",
        "  if add_data:\n",
        "      old_results_df = pd.read_csv(output_file)\n",
        "      total_results_df = pd.concat([old_results_df, results_df], ignore_index=True)\n",
        "  else:\n",
        "      total_results_df = results_df\n",
        "\n",
        "  total_results_df.to_csv(output_file, index=False)\n",
        "  print(f\"Total rows: {len(total_results_df)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6d_bpnFR7SNS",
        "outputId": "bf85f2b7-d8cf-4534-9cce-2b147c27d426"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using all available CPU cores: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Running in parallel: 100%|██████████| 2000/2000 [1:20:10<00:00,  2.41s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total rows: 40000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TD Agent"
      ],
      "metadata": {
        "id": "xLOuDhY08iXf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "simulate = True\n",
        "if simulate:\n",
        "  add_data = False\n",
        "  n_iterations = 10000\n",
        "  # Report CPU info\n",
        "  n_cores = cpu_count()\n",
        "  print(f\"Using all available CPU cores: {n_cores}\")\n",
        "\n",
        "  # Define output columns\n",
        "  column_names = [\n",
        "      'iteration', 'n_signaling_actions', 'n_final_actions', 'full_information', 'with_signals',\n",
        "      'Agent_0_Initial_NMI', 'Agent_0_NMI', 'Agent_0_avg_reward', 'Agent_0_final_reward',\n",
        "      'Agent_1_Initial_NMI', 'Agent_1_NMI', 'Agent_1_avg_reward', 'Agent_1_final_reward'\n",
        "  ]\n",
        "\n",
        "  # Global params\n",
        "  n_episodes = 10000\n",
        "  n_agents = 2\n",
        "  n_features = 2\n",
        "  n_signaling_actions = 2\n",
        "  n_final_actions = 4\n",
        "\n",
        "  def run_single_case(iteration, full_info, with_signals, game_dicts, obs_vars, graph):\n",
        "            #set seeds\n",
        "      np.random.seed(iteration)\n",
        "      random.seed(iteration)\n",
        "        # continue\n",
        "      env = TempNetMultiAgentEnv(\n",
        "          n_agents=n_agents,\n",
        "          n_features=n_features,\n",
        "          n_signaling_actions=n_signaling_actions,\n",
        "          n_final_actions=n_final_actions,\n",
        "          learning_rate=0.1,\n",
        "          exploration_rate=1.0,\n",
        "          exploration_decay=0.995,\n",
        "          min_exploration_rate=0.001,\n",
        "          full_information=full_info,\n",
        "          game_dicts=game_dicts,\n",
        "          observed_variables=obs_vars,\n",
        "          agent_type=TDLearningAgent,\n",
        "          graph=graph\n",
        "      )\n",
        "\n",
        "      env.agents = [\n",
        "              TDLearningAgent(\n",
        "                  n_actions=env.max_actions,\n",
        "                  learning_rate=0.1,  # Fixed learning rate\n",
        "                  exploration_rate=0.1,\n",
        "                  exploration_decay=0.9999999,\n",
        "                  min_exploration_rate=1e-10,\n",
        "                  gamma=0.9999999,\n",
        "                  choice='ucb'\n",
        "              ) for _ in range(n_agents)\n",
        "          ]\n",
        "\n",
        "      results = [iteration, n_signaling_actions, n_final_actions, full_info, with_signals]\n",
        "\n",
        "      signal_usage, rewards_history, signal_information_history, _, _ = temp_simulation_function(\n",
        "          n_agents=n_agents,\n",
        "          n_features=n_features,\n",
        "          n_signaling_actions=n_signaling_actions,\n",
        "          n_final_actions=n_final_actions,\n",
        "          n_episodes=n_episodes,\n",
        "          with_signals=with_signals,\n",
        "          plot=False,\n",
        "          env=env,\n",
        "          verbose=False\n",
        "      )\n",
        "\n",
        "      for agent_id in range(n_agents):\n",
        "          info_hist = signal_information_history[agent_id]\n",
        "          reward_hist = rewards_history[agent_id]\n",
        "          results.extend([\n",
        "              np.mean(info_hist[:10]),\n",
        "              np.mean(info_hist[-100:]),\n",
        "              np.mean(reward_hist),\n",
        "              np.mean(reward_hist[-100:])\n",
        "          ])\n",
        "\n",
        "      return results\n",
        "\n",
        "  def run_all_cases_for_iteration(iteration):\n",
        "\n",
        "      # Create fresh game and graph for this iteration\n",
        "      game_dicts = {i: create_random_canonical_game(n_features, n_final_actions, n=1, m=0) for i in range(n_agents)}\n",
        "      obs_vars = {0: [0], 1: [1]}\n",
        "      G = nx.DiGraph()\n",
        "      G.add_edges_from([(0, 1), (1, 0)])\n",
        "\n",
        "      cases = [(False, False), (False, True), (True, False), (True, True)]\n",
        "      return [run_single_case(iteration, fi, ws, game_dicts, obs_vars, G) for fi, ws in cases]\n",
        "\n",
        "  # Run all in parallel\n",
        "  all_results = Parallel(n_jobs=n_cores)(\n",
        "      delayed(run_all_cases_for_iteration)(i) for i in tqdm(range(n_iterations), desc=\"Running parallel simulations\")\n",
        "  )\n",
        "\n",
        "  # Flatten results\n",
        "  flat_results = [row for group in all_results for row in group]\n",
        "  results_df = pd.DataFrame(flat_results, columns=column_names)\n",
        "\n",
        "  # Save or append to file\n",
        "  output_file = dump_path+'td_learning_results_cannonical.csv'\n",
        "  if add_data:\n",
        "      old_results_df = pd.read_csv(output_file)\n",
        "      total_results_df = pd.concat([old_results_df, results_df], ignore_index=True)\n",
        "  else:\n",
        "      total_results_df = results_df\n",
        "\n",
        "  total_results_df.to_csv(output_file, index=False)\n",
        "  print(f\"Total rows in saved file: {len(total_results_df)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drUE71FK8jSP",
        "outputId": "e3ff21de-e51c-45c5-9960-92c128afbb10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using all available CPU cores: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Running parallel simulations: 100%|██████████| 10000/10000 [12:10:07<00:00,  4.38s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total rows in saved file: 40000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# More Complex Model\n",
        "\n",
        "- World States: Three binary variables X, Y, Z\n",
        "- agents_observed_variables = {0:[0,1],1:[1,2]}\n",
        "- n_features = 3 #parameters['n_features']\n",
        "- n_signaling_actions = 4 #parameters['n_signaling_actions']\n",
        "- n_final_actions = 8 #parameters['n_final_actions']\n",
        "- Random Games (possibly non-cannonical)"
      ],
      "metadata": {
        "id": "ntQk17Vm9vnu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Urn Agent"
      ],
      "metadata": {
        "id": "XH_MrAiF-G2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "simulate=False\n",
        "if simulate:\n",
        "  add_data = False\n",
        "  n_iterations = 10000\n",
        "\n",
        "  # Report available CPU cores\n",
        "  n_cores = cpu_count()\n",
        "  print(f\"Using all available CPU cores: {n_cores}\")\n",
        "\n",
        "  # Define column names\n",
        "  column_names = [\n",
        "      'iteration', 'n_signaling_actions', 'n_final_actions', 'full_information', 'with_signals',\n",
        "      'Agent_0_Initial_NMI', 'Agent_0_NMI', 'Agent_0_avg_reward', 'Agent_0_final_reward',\n",
        "      'Agent_1_Initial_NMI', 'Agent_1_NMI', 'Agent_1_avg_reward', 'Agent_1_final_reward'\n",
        "  ]\n",
        "\n",
        "  # Simulation parameters\n",
        "  n_episodes = 10000\n",
        "  n_agents = 2\n",
        "  n_features = 3\n",
        "  n_signaling_actions = 4\n",
        "  n_final_actions = 8\n",
        "\n",
        "  def run_single_case(iteration, full_info, with_signals, game_dicts, obs_vars, graph):\n",
        "\n",
        "                #set seeds\n",
        "      np.random.seed(iteration)\n",
        "      random.seed(iteration)\n",
        "      # continue\n",
        "      env = NetMultiAgentEnv(\n",
        "          n_agents=n_agents,\n",
        "          n_features=n_features,\n",
        "          n_signaling_actions=n_signaling_actions,\n",
        "          n_final_actions=n_final_actions,\n",
        "          full_information=full_info,\n",
        "          game_dicts=game_dicts,\n",
        "          observed_variables=obs_vars,\n",
        "          agent_type=UrnAgent,\n",
        "          initialize=False,\n",
        "          graph=graph\n",
        "      )\n",
        "\n",
        "      results = [iteration, n_signaling_actions, n_final_actions, full_info, with_signals]\n",
        "\n",
        "      signal_usage, rewards_history, signal_information_history, _, _ = simulation_function(\n",
        "          n_agents=n_agents,\n",
        "          n_features=n_features,\n",
        "          n_signaling_actions=n_signaling_actions,\n",
        "          n_final_actions=n_final_actions,\n",
        "          n_episodes=n_episodes,\n",
        "          with_signals=with_signals,\n",
        "          plot=False,\n",
        "          env=env,\n",
        "          verbose=False\n",
        "      )\n",
        "\n",
        "      for agent_id in range(n_agents):\n",
        "          info_hist = signal_information_history[agent_id]\n",
        "          reward_hist = rewards_history[agent_id]\n",
        "          results.extend([\n",
        "              np.mean(info_hist[:10]),\n",
        "              np.mean(info_hist[-100:]),\n",
        "              np.mean(reward_hist),\n",
        "              np.mean(reward_hist[-100:])\n",
        "          ])\n",
        "\n",
        "      return results\n",
        "\n",
        "  def run_all_cases_for_iteration(iteration):\n",
        "\n",
        "      game_dicts = {i: create_random_canonical_game(n_features, n_final_actions, n=1, m=0) for i in range(n_agents)}\n",
        "      obs_vars = {0:[0,1],1:[1,2]}\n",
        "      G = nx.DiGraph()\n",
        "      G.add_edges_from([(0, 1), (1, 0)])\n",
        "\n",
        "      cases = [(False, False), (False, True), (True, False), (True, True)]\n",
        "      return [run_single_case(iteration, fi, ws, game_dicts, obs_vars, G) for fi, ws in cases]\n",
        "\n",
        "  # Run in parallel\n",
        "  all_results = Parallel(n_jobs=n_cores)(\n",
        "      delayed(run_all_cases_for_iteration)(i) for i in tqdm(range(n_iterations), desc=\"Running UrnAgent simulations\")\n",
        "  )\n",
        "\n",
        "  # Flatten results and create DataFrame\n",
        "  flat_results = [row for group in all_results for row in group]\n",
        "  results_df = pd.DataFrame(flat_results, columns=column_names)\n",
        "\n",
        "  # Append or save\n",
        "  output_file = dump_path+'urnagent_results_complex.csv'\n",
        "  if add_data:\n",
        "      old_results_df = pd.read_csv(output_file)\n",
        "      total_results_df = pd.concat([old_results_df, results_df], ignore_index=True)\n",
        "  else:\n",
        "      total_results_df = results_df\n",
        "\n",
        "  total_results_df.to_csv(output_file, index=False)\n",
        "  print(f\"Total rows in saved file: {len(total_results_df)}\")"
      ],
      "metadata": {
        "id": "ViRgyORJ-G2w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66bce01e-eafc-447b-ca9d-6471cc3094c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using all available CPU cores: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Running UrnAgent simulations: 100%|██████████| 10000/10000 [16:24:38<00:00,  5.91s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total rows in saved file: 40000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q-Learning"
      ],
      "metadata": {
        "id": "_vJ62NkB-G2w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# n_iterations = 10\n",
        "# Print number of available CPU cores\n",
        "n_cores = cpu_count()\n",
        "print(f\"Using all available CPU cores: {n_cores}\")\n",
        "\n",
        "# Define column names\n",
        "column_names = [\n",
        "    'iteration', 'n_signaling_actions', 'n_final_actions', 'full_information', 'with_signals',\n",
        "    'Agent_0_Initial_NMI', 'Agent_0_NMI', 'Agent_0_avg_reward', 'Agent_0_final_reward',\n",
        "    'Agent_1_Initial_NMI', 'Agent_1_NMI', 'Agent_1_avg_reward', 'Agent_1_final_reward'\n",
        "]\n",
        "\n",
        "n_episodes = 1000\n",
        "n_agents = 2\n",
        "n_features = 3\n",
        "n_signaling_actions = 4\n",
        "n_final_actions = 8\n",
        "\n",
        "def run_single_case(iteration, full_info, with_signals, game_dicts, obs_vars, graph):\n",
        "\n",
        "            #set seeds\n",
        "    np.random.seed(iteration)\n",
        "    random.seed(iteration)\n",
        "      # continue\n",
        "    env = NetMultiAgentEnv(\n",
        "        n_agents=n_agents,\n",
        "        n_features=n_features,\n",
        "        n_signaling_actions=n_signaling_actions,\n",
        "        n_final_actions=n_final_actions,\n",
        "        full_information=full_info,\n",
        "        game_dicts=game_dicts,\n",
        "        observed_variables=obs_vars,\n",
        "        agent_type=QLearningAgent,\n",
        "        initialize=False,\n",
        "        graph=graph\n",
        "    )\n",
        "\n",
        "    env.agents = [\n",
        "            QLearningAgent(\n",
        "                n_signaling_actions=n_signaling_actions,\n",
        "                n_final_actions=n_final_actions,\n",
        "                exploration_rate=0.7366904308431645,\n",
        "                exploration_decay=0.9999999,\n",
        "                min_exploration_rate=0.0001,\n",
        "                choice = 'ucb'\n",
        "            ) for _ in range(n_agents)\n",
        "        ]\n",
        "\n",
        "    results = [iteration, n_signaling_actions, n_final_actions, full_info, with_signals]\n",
        "\n",
        "    signal_usage, rewards_history, signal_information_history, *_ = simulation_function(\n",
        "        n_agents=n_agents,\n",
        "        n_features=n_features,\n",
        "        n_signaling_actions=n_signaling_actions,\n",
        "        n_final_actions=n_final_actions,\n",
        "        n_episodes=n_episodes,\n",
        "        with_signals=with_signals,\n",
        "        plot=False,\n",
        "        env=env,\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    for agent_id in range(n_agents):\n",
        "        info_hist = signal_information_history[agent_id]\n",
        "        reward_hist = rewards_history[agent_id]\n",
        "        results.extend([\n",
        "            np.mean(info_hist[:10]),\n",
        "            np.mean(info_hist[-100:]),\n",
        "            np.mean(reward_hist),\n",
        "            np.mean(reward_hist[-100:])\n",
        "        ])\n",
        "\n",
        "    return results\n",
        "\n",
        "def run_all_cases_for_iteration(iteration):\n",
        "    # Prepare shared data for all 4 cases\n",
        "    game_dicts = {i: create_random_canonical_game(n_features, n_final_actions) for i in range(n_agents)}\n",
        "    obs_vars = {0:[0,1],1:[1,2]}\n",
        "    G = nx.DiGraph()\n",
        "    G.add_edges_from([(0, 1), (1, 0)])\n",
        "\n",
        "    cases = [(False, False), (False, True), (True, False), (True, True)]\n",
        "    return [run_single_case(iteration, fi, ws, game_dicts, obs_vars, G) for fi, ws in cases]\n",
        "\n",
        "# Run simulations in parallel using all available cores\n",
        "all_results = Parallel(n_jobs=n_cores)(\n",
        "    delayed(run_all_cases_for_iteration)(i) for i in tqdm(range(n_iterations), desc=\"Running in parallel\")\n",
        ")\n",
        "\n",
        "# Flatten the list of lists\n",
        "flat_results = [row for group in all_results for row in group]\n",
        "results_df = pd.DataFrame(flat_results, columns=column_names)\n",
        "\n",
        "# Append or save\n",
        "output_file = dump_path+'qlearning_results_complex.csv'\n",
        "if add_data:\n",
        "    old_results_df = pd.read_csv(output_file)\n",
        "    total_results_df = pd.concat([old_results_df, results_df], ignore_index=True)\n",
        "else:\n",
        "    total_results_df = results_df\n",
        "\n",
        "total_results_df.to_csv(output_file, index=False)\n",
        "print(f\"Total rows: {len(total_results_df)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a85dc6f8-897d-4a2e-c93e-e0358cb5e825",
        "id": "cDETgMDw-G2x"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using all available CPU cores: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Running in parallel: 100%|██████████| 2000/2000 [24:51<00:00,  1.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total rows: 8000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TD Agent"
      ],
      "metadata": {
        "id": "grczen2I-G2x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# n_iterations = 10\n",
        "# Report CPU info\n",
        "n_cores = cpu_count()\n",
        "print(f\"Using all available CPU cores: {n_cores}\")\n",
        "\n",
        "# Define output columns\n",
        "column_names = [\n",
        "    'iteration', 'n_signaling_actions', 'n_final_actions', 'full_information', 'with_signals',\n",
        "    'Agent_0_Initial_NMI', 'Agent_0_NMI', 'Agent_0_avg_reward', 'Agent_0_final_reward',\n",
        "    'Agent_1_Initial_NMI', 'Agent_1_NMI', 'Agent_1_avg_reward', 'Agent_1_final_reward'\n",
        "]\n",
        "\n",
        "# Global params\n",
        "n_episodes = 1000\n",
        "n_agents = 2\n",
        "n_features = 3\n",
        "n_signaling_actions = 4\n",
        "n_final_actions = 8\n",
        "\n",
        "def run_single_case(iteration, full_info, with_signals, game_dicts, obs_vars, graph):\n",
        "\n",
        "    #set seeds\n",
        "    np.random.seed(iteration)\n",
        "    random.seed(iteration)\n",
        "    # continue\n",
        "    env = TempNetMultiAgentEnv(\n",
        "        n_agents=n_agents,\n",
        "        n_features=n_features,\n",
        "        n_signaling_actions=n_signaling_actions,\n",
        "        n_final_actions=n_final_actions,\n",
        "        learning_rate=0.1,\n",
        "        exploration_rate=1.0,\n",
        "        exploration_decay=0.995,\n",
        "        min_exploration_rate=0.001,\n",
        "        full_information=full_info,\n",
        "        game_dicts=game_dicts,\n",
        "        observed_variables=obs_vars,\n",
        "        agent_type=TDLearningAgent,\n",
        "        graph=graph\n",
        "    )\n",
        "\n",
        "    env.agents = [\n",
        "            TDLearningAgent(\n",
        "                n_actions=env.max_actions,\n",
        "                learning_rate=0.1,  # Fixed learning rate\n",
        "                exploration_rate=1,\n",
        "                exploration_decay=0.995,\n",
        "                min_exploration_rate=0.0001,\n",
        "                gamma=0.99,\n",
        "                choice = 'egreedy'\n",
        "            ) for _ in range(n_agents)\n",
        "        ]\n",
        "\n",
        "    results = [iteration, n_signaling_actions, n_final_actions, full_info, with_signals]\n",
        "\n",
        "    signal_usage, rewards_history, signal_information_history, _, _ = temp_simulation_function(\n",
        "        n_agents=n_agents,\n",
        "        n_features=n_features,\n",
        "        n_signaling_actions=n_signaling_actions,\n",
        "        n_final_actions=n_final_actions,\n",
        "        n_episodes=n_episodes,\n",
        "        with_signals=with_signals,\n",
        "        plot=False,\n",
        "        env=env,\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    for agent_id in range(n_agents):\n",
        "        info_hist = signal_information_history[agent_id]\n",
        "        reward_hist = rewards_history[agent_id]\n",
        "        results.extend([\n",
        "            np.mean(info_hist[:10]),\n",
        "            np.mean(info_hist[-100:]),\n",
        "            np.mean(reward_hist),\n",
        "            np.mean(reward_hist[-100:])\n",
        "        ])\n",
        "\n",
        "    return results\n",
        "\n",
        "def run_all_cases_for_iteration(iteration):\n",
        "\n",
        "    # Create fresh game and graph for this iteration\n",
        "    game_dicts = {i: create_random_canonical_game(n_features, n_final_actions, n=1, m=0) for i in range(n_agents)}\n",
        "    obs_vars = {0:[0,1],1:[1,2]}\n",
        "    G = nx.DiGraph()\n",
        "    G.add_edges_from([(0, 1), (1, 0)])\n",
        "\n",
        "    cases = [(False, False), (False, True), (True, False), (True, True)]\n",
        "    return [run_single_case(iteration, fi, ws, game_dicts, obs_vars, G) for fi, ws in cases]\n",
        "\n",
        "# Run all in parallel\n",
        "all_results = Parallel(n_jobs=n_cores)(\n",
        "    delayed(run_all_cases_for_iteration)(i) for i in tqdm(range(n_iterations), desc=\"Running parallel simulations\")\n",
        ")\n",
        "\n",
        "# Flatten results\n",
        "flat_results = [row for group in all_results for row in group]\n",
        "results_df = pd.DataFrame(flat_results, columns=column_names)\n",
        "\n",
        "# Save or append to file\n",
        "output_file = dump_path+'td_learning_results_complex.csv'\n",
        "if add_data:\n",
        "    old_results_df = pd.read_csv(output_file)\n",
        "    total_results_df = pd.concat([old_results_df, results_df], ignore_index=True)\n",
        "else:\n",
        "    total_results_df = results_df\n",
        "\n",
        "total_results_df.to_csv(output_file, index=False)\n",
        "print(f\"Total rows in saved file: {len(total_results_df)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "811b52f5-feaf-4081-8556-58aab64da145",
        "id": "NVFoEBoC-G2y"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using all available CPU cores: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Running parallel simulations: 100%|██████████| 10/10 [00:00<00:00, 6440.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total rows in saved file: 40\n"
          ]
        }
      ]
    }
  ]
}