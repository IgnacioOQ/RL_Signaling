{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imports import *\n",
    "from utils import *\n",
    "from agents import UrnAgent\n",
    "from environment import MultiAgentEnv\n",
    "from simulation_functions import simulation_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cannonical Model\n",
    "\n",
    "- World States: Two binary variables X, Y\n",
    "- agents_observed_variables = {0:[0],1:[1]}\n",
    "- Random Cannonical Games\n",
    "- n_features = 2 \n",
    "- n_signaling_actions = 2 \n",
    "- n_final_actions = 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing processing simulations...: 100%|██████████| 1/1 [00:24<00:00, 24.28s/it]\n"
     ]
    }
   ],
   "source": [
    "# Define column names\n",
    "column_names = ['iteration','n_signaling_actions','n_final_actions','full_information','with_signals',\n",
    "                'Agent_0_Initial_NMI','Agent_0_NMI', 'Agent_0_avg_reward', 'Agent_0_final_reward',\n",
    "                'Agent_1_Initial_NMI','Agent_1_NMI', 'Agent_1_avg_reward','Agent_1_final_reward']\n",
    "\n",
    "# Create an empty DataFrame with the specified column names\n",
    "results_df = pd.DataFrame(columns=column_names)\n",
    "\n",
    "n_episodes = 10000\n",
    "n_iterations = 1000\n",
    "\n",
    "#for iterations in tqdm(range(n_iterations), desc=\"Processing\"):\n",
    "for iteration in tqdm(range(n_iterations), desc=\"Processing processing simulations...\"):\n",
    "    n_agents = 2 \n",
    "    n_features = 2 \n",
    "    n_signaling_actions = 2 \n",
    "    n_final_actions = 4 \n",
    "\n",
    "    # We get the same dictionary of games for each of the four potential setups/cases\n",
    "    # but agents have play distinct independent games at each iteration\n",
    "    randomcannonical_game = {}\n",
    "    for i in range(n_agents):\n",
    "        randomcannonical_game[i] = create_randomcannonical_game(n_features,n_final_actions)\n",
    "\n",
    "    # Similarly for each of the four potential setups we keep fixed which variables are observed by each agent\n",
    "    # a dictionary of lists of the observed indexed variables\n",
    "    agents_observed_variables = {0:[0],1:[1]}\n",
    "\n",
    "    # CASE 1 , self.signal_information_history\n",
    "    with_signals,full_information = False, False\n",
    "    env = MultiAgentEnv(n_agents=n_agents, n_features=n_features,\n",
    "                    n_signaling_actions=n_signaling_actions,\n",
    "                    n_final_actions=n_final_actions,\n",
    "                    full_information = full_information,\n",
    "                    game_dicts=randomcannonical_game,\n",
    "                    observed_variables = agents_observed_variables)\n",
    "    results = [iteration,n_signaling_actions,n_final_actions,full_information,with_signals]\n",
    "    signal_usage, rewards_history, signal_information_history, urn_histories,nature_history = simulation_function(n_agents=n_agents,\n",
    "                        n_features=n_features, n_signaling_actions=n_signaling_actions, n_final_actions=n_final_actions,\n",
    "                        n_episodes=n_episodes, with_signals = with_signals,\n",
    "                        plot=False,env=env,initialize = False, verbose=False)\n",
    "\n",
    "    # mutual_info_0, normalized_mutual_info_0 = compute_mutual_information(signal_usage[0])\n",
    "    # mutual_info_1, normalized_mutual_info_1 = compute_mutual_information(signal_usage[1])\n",
    "    output = [np.mean(signal_information_history[0][:10]),np.mean(signal_information_history[0][-100:]),\n",
    "                np.mean(rewards_history[0]),np.mean(rewards_history[0][-100:]),\n",
    "                np.mean(signal_information_history[1][:10]),np.mean(signal_information_history[1][-100:]),\n",
    "                np.mean(rewards_history[1]),np.mean(rewards_history[1][-100:])]\n",
    "    results+=output\n",
    "    # Adding the list as a row using loc\n",
    "    results_df.loc[len(results_df)] = results\n",
    "\n",
    "    # CASE 2\n",
    "    with_signals,full_information = True, False\n",
    "    env = MultiAgentEnv(n_agents=n_agents, n_features=n_features,\n",
    "                    n_signaling_actions=n_signaling_actions,\n",
    "                    n_final_actions=n_final_actions,\n",
    "                    full_information = full_information,\n",
    "                    game_dicts=randomcannonical_game,\n",
    "                    observed_variables = agents_observed_variables)\n",
    "    results = [iteration,n_signaling_actions,n_final_actions,full_information,with_signals]\n",
    "    signal_usage, rewards_history, signal_information_history, urn_histories,nature_history = simulation_function(n_agents=n_agents,\n",
    "                        n_features=n_features, n_signaling_actions=n_signaling_actions, n_final_actions=n_final_actions,\n",
    "                        n_episodes=n_episodes, with_signals = with_signals,\n",
    "                        plot=False,env=env,initialize = False, verbose=False)\n",
    "\n",
    "    # mutual_info_0, normalized_mutual_info_0 = compute_mutual_information(signal_usage[0])\n",
    "    # mutual_info_1, normalized_mutual_info_1 = compute_mutual_information(signal_usage[1])\n",
    "    output = [np.mean(signal_information_history[0][:10]),np.mean(signal_information_history[0][-100:]),\n",
    "                np.mean(rewards_history[0]),np.mean(rewards_history[0][-100:]),\n",
    "                np.mean(signal_information_history[1][:10]),np.mean(signal_information_history[1][-100:]),\n",
    "                np.mean(rewards_history[1]),np.mean(rewards_history[1][-100:])]\n",
    "    results+=output\n",
    "    # Adding the list as a row using loc\n",
    "    results_df.loc[len(results_df)] = results\n",
    "\n",
    "    # CASE 3\n",
    "    with_signals,full_information = False, True\n",
    "    env = MultiAgentEnv(n_agents=n_agents, n_features=n_features,\n",
    "                    n_signaling_actions=n_signaling_actions,\n",
    "                    n_final_actions=n_final_actions,\n",
    "                    full_information = full_information,\n",
    "                    game_dicts=randomcannonical_game,\n",
    "                    observed_variables = agents_observed_variables)\n",
    "    results = [iteration,n_signaling_actions,n_final_actions,full_information,with_signals]\n",
    "    signal_usage, rewards_history, signal_information_history, urn_histories,nature_history = simulation_function(n_agents=n_agents,\n",
    "                        n_features=n_features, n_signaling_actions=n_signaling_actions, n_final_actions=n_final_actions,\n",
    "                        n_episodes=n_episodes, with_signals = with_signals,\n",
    "                        plot=False,env=env,initialize = False, verbose=False)\n",
    "\n",
    "    output = [np.mean(signal_information_history[0][:10]),np.mean(signal_information_history[0][-100:]),\n",
    "                np.mean(rewards_history[0]),np.mean(rewards_history[0][-100:]),\n",
    "                np.mean(signal_information_history[1][:10]),np.mean(signal_information_history[1][-100:]),\n",
    "                np.mean(rewards_history[1]),np.mean(rewards_history[1][-100:])]\n",
    "    results+=output\n",
    "    # Adding the list as a row using loc\n",
    "    results_df.loc[len(results_df)] = results\n",
    "\n",
    "    # CASE 4\n",
    "    with_signals,full_information = True, True\n",
    "    env = MultiAgentEnv(n_agents=n_agents, n_features=n_features,\n",
    "                    n_signaling_actions=n_signaling_actions,\n",
    "                    n_final_actions=n_final_actions,\n",
    "                    full_information = full_information,\n",
    "                    game_dicts=randomcannonical_game,\n",
    "                    observed_variables = agents_observed_variables)\n",
    "    results = [iteration,n_signaling_actions,n_final_actions,full_information,with_signals]\n",
    "    signal_usage, rewards_history, signal_information_history, urn_histories,nature_history = simulation_function(n_agents=n_agents,\n",
    "                        n_features=n_features, n_signaling_actions=n_signaling_actions, n_final_actions=n_final_actions,\n",
    "                        n_episodes=n_episodes, with_signals = with_signals,\n",
    "                        plot=False,env=env,initialize = False, verbose=False)\n",
    "\n",
    "    # mutual_info_0, normalized_mutual_info_0 = compute_mutual_information(signal_usage[0])\n",
    "    # mutual_info_1, normalized_mutual_info_1 = compute_mutual_information(signal_usage[1])\n",
    "    output = [np.mean(signal_information_history[0][:10]),np.mean(signal_information_history[0][-100:]),\n",
    "                np.mean(rewards_history[0]),np.mean(rewards_history[0][-100:]),\n",
    "                np.mean(signal_information_history[1][:10]),np.mean(signal_information_history[1][-100:]),\n",
    "                np.mean(rewards_history[1]),np.mean(rewards_history[1][-100:])]\n",
    "    results+=output\n",
    "    # Adding the list as a row using loc\n",
    "    results_df.loc[len(results_df)] = results\n",
    "\n",
    "results_df.to_csv('basic_urn_results_cannonical.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Complex Model\n",
    "\n",
    "- World States: Three binary variables X, Y, Z\n",
    "- agents_observed_variables = {0:[0,1],1:[1,2]}\n",
    "- n_features = 3 #parameters['n_features']\n",
    "- n_signaling_actions = 4 #parameters['n_signaling_actions']\n",
    "- n_final_actions = 8 #parameters['n_final_actions']\n",
    "- Random Games (possibly non-cannonical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing processing simulations:   0%|          | 1/1000 [00:50<13:55:48, 50.20s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 38\u001b[0m\n\u001b[1;32m     31\u001b[0m env \u001b[38;5;241m=\u001b[39m MultiAgentEnv(n_agents\u001b[38;5;241m=\u001b[39mn_agents, n_features\u001b[38;5;241m=\u001b[39mn_features,\n\u001b[1;32m     32\u001b[0m                 n_signaling_actions\u001b[38;5;241m=\u001b[39mn_signaling_actions,\n\u001b[1;32m     33\u001b[0m                 n_final_actions\u001b[38;5;241m=\u001b[39mn_final_actions,\n\u001b[1;32m     34\u001b[0m                 full_information \u001b[38;5;241m=\u001b[39m full_information,\n\u001b[1;32m     35\u001b[0m                 game_dicts\u001b[38;5;241m=\u001b[39mrandom_game,\n\u001b[1;32m     36\u001b[0m                 observed_variables \u001b[38;5;241m=\u001b[39m agents_observed_variables)\n\u001b[1;32m     37\u001b[0m results \u001b[38;5;241m=\u001b[39m [iteration,n_signaling_actions,n_final_actions,full_information,with_signals]\n\u001b[0;32m---> 38\u001b[0m signal_usage, rewards_history, signal_information_history, urn_histories,nature_history \u001b[38;5;241m=\u001b[39m \u001b[43msimulation_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_agents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_agents\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mn_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_signaling_actions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_signaling_actions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_final_actions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_final_actions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mn_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_episodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwith_signals\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mwith_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mplot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43minitialize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m output \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mmean(signal_information_history[\u001b[38;5;241m0\u001b[39m][:\u001b[38;5;241m10\u001b[39m]),np\u001b[38;5;241m.\u001b[39mmean(signal_information_history[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m:]),\n\u001b[1;32m     44\u001b[0m           np\u001b[38;5;241m.\u001b[39mmean(rewards_history[\u001b[38;5;241m0\u001b[39m]),np\u001b[38;5;241m.\u001b[39mmean(rewards_history[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m:]),\n\u001b[1;32m     45\u001b[0m           np\u001b[38;5;241m.\u001b[39mmean(signal_information_history[\u001b[38;5;241m1\u001b[39m][:\u001b[38;5;241m10\u001b[39m]),np\u001b[38;5;241m.\u001b[39mmean(signal_information_history[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m:]),\n\u001b[1;32m     46\u001b[0m           np\u001b[38;5;241m.\u001b[39mmean(rewards_history[\u001b[38;5;241m1\u001b[39m]),np\u001b[38;5;241m.\u001b[39mmean(rewards_history[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m:])]\n\u001b[1;32m     47\u001b[0m results\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39moutput\n",
      "File \u001b[0;32m~/GitRepositories/RL_Signaling/simulation_functions.py:59\u001b[0m, in \u001b[0;36msimulation_function\u001b[0;34m(n_agents, n_features, n_signaling_actions, n_final_actions, n_episodes, with_signals, plot, env, agent_type, initialize, verbose)\u001b[0m\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124magents direct observations are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00magents_observations\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Step 0: Agents choose signaling actions based on Q-learning policy\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m signals \u001b[38;5;241m=\u001b[39m [agent\u001b[38;5;241m.\u001b[39mget_action(observation, is_signaling\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m agent, observation \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43magents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magents_observations\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# step to store signaling history, and move to the next step in the episode\u001b[39;00m\n\u001b[1;32m     61\u001b[0m _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(signals)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define column names\n",
    "column_names = ['iteration','n_signaling_actions','n_final_actions','full_information','with_signals',\n",
    "                'Agent_0_Initial_NMI','Agent_0_NMI', 'Agent_0_avg_reward', 'Agent_0_final_reward',\n",
    "                'Agent_1_Initial_NMI','Agent_1_NMI', 'Agent_1_avg_reward','Agent_1_final_reward']\n",
    "\n",
    "# Create an empty DataFrame with the specified column names\n",
    "results_df = pd.DataFrame(columns=column_names)\n",
    "\n",
    "n_episodes = 10000\n",
    "n_iterations = 500\n",
    "\n",
    "#for iterations in tqdm(range(n_iterations), desc=\"Processing\"):\n",
    "for iteration in tqdm(range(n_iterations), desc=\"Processing processing simulations\"):\n",
    "    # get the paramteres from the dictionary\n",
    "    n_agents = 2 #parameters['n_agents']\n",
    "    n_features = 3 #parameters['n_features']\n",
    "    n_signaling_actions = 4 #parameters['n_signaling_actions']\n",
    "    n_final_actions = 8 #parameters['n_final_actions']\n",
    "\n",
    "# We get the same dictionary of games for each of the four potential setups/cases\n",
    "    # but agents have play distinct independent games at each iteration\n",
    "    random_game = {}\n",
    "    for i in range(n_agents):\n",
    "        random_game[i] = create_random_game(n_features,n_final_actions)\n",
    "    # Similarly for each of the four potential setups we keep fixed which variables are observed by each agent\n",
    "    # a dictionary of lists of the observed indexed variables\n",
    "    agents_observed_variables = {0:[0,1],1:[1,2]}\n",
    "    \n",
    "    # CASE 1 , self.signal_information_history\n",
    "    with_signals,full_information = False, False\n",
    "    env = MultiAgentEnv(n_agents=n_agents, n_features=n_features,\n",
    "                    n_signaling_actions=n_signaling_actions,\n",
    "                    n_final_actions=n_final_actions,\n",
    "                    full_information = full_information,\n",
    "                    game_dicts=random_game,\n",
    "                    observed_variables = agents_observed_variables)\n",
    "    results = [iteration,n_signaling_actions,n_final_actions,full_information,with_signals]\n",
    "    signal_usage, rewards_history, signal_information_history, urn_histories,nature_history = simulation_function(n_agents=n_agents,\n",
    "                        n_features=n_features, n_signaling_actions=n_signaling_actions, n_final_actions=n_final_actions,\n",
    "                        n_episodes=n_episodes, with_signals = with_signals,\n",
    "                        plot=False,env=env,initialize = False, verbose=False)\n",
    " \n",
    "    output = [np.mean(signal_information_history[0][:10]),np.mean(signal_information_history[0][-100:]),\n",
    "              np.mean(rewards_history[0]),np.mean(rewards_history[0][-100:]),\n",
    "              np.mean(signal_information_history[1][:10]),np.mean(signal_information_history[1][-100:]),\n",
    "              np.mean(rewards_history[1]),np.mean(rewards_history[1][-100:])]\n",
    "    results+=output\n",
    "    # Adding the list as a row using loc\n",
    "    results_df.loc[len(results_df)] = results\n",
    "\n",
    "    # CASE 2\n",
    "    with_signals,full_information = True, False\n",
    "    env = MultiAgentEnv(n_agents=n_agents, n_features=n_features,\n",
    "                    n_signaling_actions=n_signaling_actions,\n",
    "                    n_final_actions=n_final_actions,\n",
    "                    full_information = full_information,\n",
    "                    game_dicts=random_game,\n",
    "                    observed_variables = agents_observed_variables)\n",
    "    results = [iteration,n_signaling_actions,n_final_actions,full_information,with_signals]\n",
    "    signal_usage, rewards_history, signal_information_history, urn_histories,nature_history = simulation_function(n_agents=n_agents,\n",
    "                        n_features=n_features, n_signaling_actions=n_signaling_actions, n_final_actions=n_final_actions,\n",
    "                        n_episodes=n_episodes, with_signals = with_signals,\n",
    "                        plot=False,env=env,initialize = False, verbose=False)\n",
    "\n",
    "    # mutual_info_0, normalized_mutual_info_0 = compute_mutual_information(signal_usage[0])\n",
    "    # mutual_info_1, normalized_mutual_info_1 = compute_mutual_information(signal_usage[1])\n",
    "    output = [np.mean(signal_information_history[0][:10]),np.mean(signal_information_history[0][-100:]),\n",
    "              np.mean(rewards_history[0]),np.mean(rewards_history[0][-100:]),\n",
    "              np.mean(signal_information_history[1][:10]),np.mean(signal_information_history[1][-100:]),\n",
    "              np.mean(rewards_history[1]),np.mean(rewards_history[1][-100:])]\n",
    "    results+=output\n",
    "    # Adding the list as a row using loc\n",
    "    results_df.loc[len(results_df)] = results\n",
    "\n",
    "    # CASE 3\n",
    "    with_signals,full_information = False, True\n",
    "    env = MultiAgentEnv(n_agents=n_agents, n_features=n_features,\n",
    "                    n_signaling_actions=n_signaling_actions,\n",
    "                    n_final_actions=n_final_actions,\n",
    "                    full_information = full_information,\n",
    "                    game_dicts=random_game,\n",
    "                    observed_variables = agents_observed_variables)\n",
    "    results = [iteration,n_signaling_actions,n_final_actions,full_information,with_signals]\n",
    "    signal_usage, rewards_history, signal_information_history, urn_histories,nature_history = simulation_function(n_agents=n_agents,\n",
    "                        n_features=n_features, n_signaling_actions=n_signaling_actions, n_final_actions=n_final_actions,\n",
    "                        n_episodes=n_episodes, with_signals = with_signals,\n",
    "                        plot=False,env=env,initialize = False, verbose=False)\n",
    "\n",
    "    # mutual_info_0, normalized_mutual_info_0 = compute_mutual_information(signal_usage[0])\n",
    "    # mutual_info_1, normalized_mutual_info_1 = compute_mutual_information(signal_usage[1])\n",
    "    output = [np.mean(signal_information_history[0][:10]),np.mean(signal_information_history[0][-100:]),\n",
    "              np.mean(rewards_history[0]),np.mean(rewards_history[0][-100:]),\n",
    "              np.mean(signal_information_history[1][:10]),np.mean(signal_information_history[1][-100:]),\n",
    "              np.mean(rewards_history[1]),np.mean(rewards_history[1][-100:])]\n",
    "    results+=output\n",
    "    # Adding the list as a row using loc\n",
    "    results_df.loc[len(results_df)] = results\n",
    "\n",
    "    # CASE 4\n",
    "    with_signals,full_information = True, True\n",
    "    env = MultiAgentEnv(n_agents=n_agents, n_features=n_features,\n",
    "                    n_signaling_actions=n_signaling_actions,\n",
    "                    n_final_actions=n_final_actions,\n",
    "                    full_information = full_information,\n",
    "                    game_dicts=random_game,\n",
    "                    observed_variables = agents_observed_variables)\n",
    "    results = [iteration,n_signaling_actions,n_final_actions,full_information,with_signals]\n",
    "    signal_usage, rewards_history, signal_information_history, urn_histories,nature_history = simulation_function(n_agents=n_agents,\n",
    "                        n_features=n_features, n_signaling_actions=n_signaling_actions, n_final_actions=n_final_actions,\n",
    "                        n_episodes=n_episodes, with_signals = with_signals,\n",
    "                        plot=False,env=env,initialize = False, verbose=False)\n",
    "\n",
    "    # mutual_info_0, normalized_mutual_info_0 = compute_mutual_information(signal_usage[0])\n",
    "    # mutual_info_1, normalized_mutual_info_1 = compute_mutual_information(signal_usage[1])\n",
    "    output = [np.mean(signal_information_history[0][:10]),np.mean(signal_information_history[0][-100:]),\n",
    "              np.mean(rewards_history[0]),np.mean(rewards_history[0][-100:]),\n",
    "              np.mean(signal_information_history[1][:10]),np.mean(signal_information_history[1][-100:]),\n",
    "              np.mean(rewards_history[1]),np.mean(rewards_history[1][-100:])]\n",
    "    results+=output\n",
    "    # Adding the list as a row using loc\n",
    "    results_df.loc[len(results_df)] = results\n",
    "\n",
    "results_df.to_csv('urn_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
