# Multi-Agent Environment
class MultiAgentEnv:
    def __init__(self, n_agents=2, n_features=5, n_signaling_actions=3, n_final_actions=2, full_information = False):
        """
        Initialize the multi-agent environment with separate signaling and final actions.

        :param n_agents: Number of agents
        :param n_features: Length of the binary vector generated by nature
        :param n_signaling_actions: Number of possible signaling actions for agents
        :param n_final_actions: Number of possible final actions for agents
        """
        self.n_agents = n_agents
        self.n_features = n_features
        self.n_signaling_actions = n_signaling_actions
        self.n_final_actions = n_final_actions
        self.current_step = 0
        self.full_information = full_information

        # Internal state variables
        self.nature_vector = None
        self.signals = None
        self.final_actions = None

        # One random game dictionary per agent
        self.internal_game_dicts = {}
        for i in range(self.n_agents):
          self.internal_game_dicts[i] = self.create_random_game()

        # Init which variables each agent observes
        self.agents_observed_variables = {}
        for i in range(self.n_agents):
          # Randomly determine the number of indexes to select (between 1 and n)
          num_indexes = random.randint(1, self.n_features)
          # Randomly sample indexes from the range [0, n-1]
          random_indexes = random.sample(range(self.n_features), num_indexes)
          # Subset the vector using the selected indexes
          #subset = [vector[i] for i in random_indexes]
          self.agents_observed_variables[i] = random_indexes

        # Store the relevant histories (signals and rewards)
        # Tracking metrics
        # # FOR EACH AGENT I WANT, FOR EACH STATE OF THE WORLD (OR OBSERVATION), WHICH SIGNAL WAS USED
        # INDEPENDENTLY I ALSO WANT THE REWARD HISTORY FOR BOTH AGENTS
        # self.world_states = set(product([0, 1], repeat=self.n_features))
        self.rewards_history = [[] for _ in range(self.n_agents)]  # Store rewards per episode
        self.signal_usage = [{} for _ in range(self.n_agents)]  # Track signal counts for each agent

    def reset(self):
        """
        Reset the environment for a new episode.

        :return: Initial observation (binary vector from nature)
        """
        self.current_step = 0
        self.nature_vector = np.random.randint(0, 2, size=self.n_features)  # Binary nature vector
        self.signals = [None] * self.n_agents  # Reset signals
        self.final_actions = [None] * self.n_agents  # Reset final actions
        return self.nature_vector

    def step(self, actions):
        """
        Execute a step in the environment based on the current phase.

        :param actions: Actions taken by the agents
        :return: Tuple (observation, rewards, done)
            - observation: The environment state observed by agents
            - rewards: A list of rewards for each agent
            - done: Boolean indicating if the episode has ended
        """

        if self.current_step == 0:
            # Step 0: Agents perform signaling actions
            self.signals = actions
            self.current_step += 1
            # update signal history
            for i in range(self.n_agents):
              agent_observation = self.assign_observations()[i]
              #agent_observation = agents_observations[i]
              if agent_observation not in self.signal_usage[i]:
                # I initialize with one so that I measure the Normalized Mutual Information NMI
                  self.signal_usage[i][agent_observation] = [1] * n_signaling_actions
              self.signal_usage[i][agent_observation][self.signals[i]] += 1
              #new_observations = [obs+(signal,) for obs,signal in zip(self.nature_vector,self.signals)]
            return self.nature_vector, [0] * self.n_agents, False

        elif self.current_step == 1:
            # Step 1: Agents perform final actions based on signals
            self.final_actions = actions
            #rewards = self._calculate_rewards()
            rewards = self.calculate_rewards()
            # Record rewards for this episode
            for i in range(self.n_agents):
              self.rewards_history[i].append(rewards[i])
            # self.current_step += 1
            return self.nature_vector, rewards, True

        else:
            raise ValueError("Environment has already completed two steps. Reset before reusing.")

    def report_metrics(self):
      # Plot results
      return self.signal_usage, self.rewards_history

    def calculate_rewards(self):
      rewards = []
      for i in range(self.n_agents):
        agent_action = self.final_actions[i]
        a_rew = self.internal_game_dicts[i][tuple(self.nature_vector)][agent_action]
        rewards.append(a_rew)
      return rewards

    def render(self):
        """
        Print the current state of the environment for debugging purposes.
        """
        print(f"Step: {self.current_step}")
        print(f"Nature Vector: {self.nature_vector}")
        print(f"Signals: {self.signals}")
        print(f"Final Actions: {self.final_actions}")

    def create_random_game(self):
      random_game_dict = dict()
      world_states = set(product([0, 1], repeat=self.n_features))
      for w in world_states:
        random_game_dict[w] = dict()
        for a in range(self.n_final_actions):
          random_game_dict[w][a] = random.randint(0, 9)
      return random_game_dict

    def assign_observations(self):
      agents_observations = []
      if self.full_information:
        for i in range(self.n_agents):
          agents_observations.append(tuple(self.nature_vector))
      else:
        for i in range(self.n_agents):
          observed_indexes = self.agents_observed_variables[i]
          vector = self.nature_vector
          subset = tuple([vector[j] for j in observed_indexes])
          agents_observations.append(subset)
      return agents_observations
