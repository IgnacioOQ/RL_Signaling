{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imports import *\n",
    "from utils import *\n",
    "from agents import QLearningAgent\n",
    "from environment import NetMultiAgentEnv\n",
    "from simulation_function import simulation_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cannonical Model\n",
    "\n",
    "- World States: Two binary variables X, Y\n",
    "- agents_observed_variables = {0:[0],1:[1]}\n",
    "- Random Cannonical Games\n",
    "- n_features = 2 \n",
    "- n_signaling_actions = 2 \n",
    "- n_final_actions = 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define column names\n",
    "column_names = ['iteration','n_signaling_actions','n_final_actions','full_information','with_signals',\n",
    "                'Agent_0_Initial_NMI','Agent_0_NMI', 'Agent_0_avg_reward', 'Agent_0_final_reward',\n",
    "                'Agent_1_Initial_NMI','Agent_1_NMI', 'Agent_1_avg_reward','Agent_1_final_reward']\n",
    "\n",
    "# Create an empty DataFrame with the specified column names\n",
    "results_df = pd.DataFrame(columns=column_names)\n",
    "\n",
    "n_episodes = 10000\n",
    "n_iterations = 980\n",
    "\n",
    "#for iterations in tqdm(range(n_iterations), desc=\"Processing\"):\n",
    "for iteration in tqdm(range(n_iterations), desc=\"Processing processing simulations...\"):\n",
    "    n_agents = 2 \n",
    "    n_features = 2 \n",
    "    n_signaling_actions = 2 \n",
    "    n_final_actions = 4 \n",
    "\n",
    "    # We get the same dictionary of games for each of the four potential setups/cases\n",
    "    # but agents have play distinct independent games at each iteration\n",
    "    randomcannonical_game = {}\n",
    "    for i in range(n_agents):\n",
    "        randomcannonical_game[i] = create_random_canonical_game(n_features,n_final_actions)\n",
    "\n",
    "    # Similarly for each of the four potential setups we keep fixed which variables are observed by each agent\n",
    "    # a dictionary of lists of the observed indexed variables\n",
    "    agents_observed_variables = {0:[0],1:[1]}\n",
    "\n",
    "    # Graph\n",
    "    G = nx.DiGraph()\n",
    "    G.add_nodes_from([0,1])  # Adds multiple nodes at once\n",
    "    G.add_edges_from([(0, 1), (1, 0)])  # Adds multiple edges\n",
    "\n",
    "\n",
    "    # CASE 1 , self.signal_information_history\n",
    "    with_signals,full_information = False, False\n",
    "    env = NetMultiAgentEnv(n_agents=n_agents, n_features=n_features,\n",
    "                  n_signaling_actions=n_signaling_actions,\n",
    "                  n_final_actions=n_final_actions,\n",
    "                  full_information = full_information,\n",
    "                  game_dicts=randomcannonical_game,\n",
    "                  observed_variables = agents_observed_variables,\n",
    "                  agent_type=QLearningAgent,\n",
    "                  initialize = False,\n",
    "                  graph=G)\n",
    "    results = [iteration,n_signaling_actions,n_final_actions,full_information,with_signals]\n",
    "    signal_usage, rewards_history, signal_information_history, urn_histories,nature_history = simulation_function(n_agents=n_agents,\n",
    "                      n_features=n_features, n_signaling_actions=n_signaling_actions, n_final_actions=n_final_actions,\n",
    "                      n_episodes=n_episodes, with_signals = with_signals,\n",
    "                      plot=False,env=env, verbose=False)\n",
    "    \n",
    "    # mutual_info_0, normalized_mutual_info_0 = compute_mutual_information(signal_usage[0])\n",
    "    # mutual_info_1, normalized_mutual_info_1 = compute_mutual_information(signal_usage[1])\n",
    "    output = [np.mean(signal_information_history[0][:10]),np.mean(signal_information_history[0][-100:]),\n",
    "                np.mean(rewards_history[0]),np.mean(rewards_history[0][-100:]),\n",
    "                np.mean(signal_information_history[1][:10]),np.mean(signal_information_history[1][-100:]),\n",
    "                np.mean(rewards_history[1]),np.mean(rewards_history[1][-100:])]\n",
    "    results+=output\n",
    "    # Adding the list as a row using loc\n",
    "    results_df.loc[len(results_df)] = results\n",
    "\n",
    "    # CASE 2\n",
    "    with_signals,full_information = True, False\n",
    "    env = NetMultiAgentEnv(n_agents=n_agents, n_features=n_features,\n",
    "                  n_signaling_actions=n_signaling_actions,\n",
    "                  n_final_actions=n_final_actions,\n",
    "                  full_information = full_information,\n",
    "                  game_dicts=randomcannonical_game,\n",
    "                  observed_variables = agents_observed_variables,\n",
    "                  agent_type=QLearningAgent,\n",
    "                  initialize = False,\n",
    "                  graph=G)\n",
    "    results = [iteration,n_signaling_actions,n_final_actions,full_information,with_signals]\n",
    "    signal_usage, rewards_history, signal_information_history, urn_histories,nature_history = simulation_function(n_agents=n_agents,\n",
    "                      n_features=n_features, n_signaling_actions=n_signaling_actions, n_final_actions=n_final_actions,\n",
    "                      n_episodes=n_episodes, with_signals = with_signals,\n",
    "                      plot=False,env=env, verbose=False)\n",
    "\n",
    "    # mutual_info_0, normalized_mutual_info_0 = compute_mutual_information(signal_usage[0])\n",
    "    # mutual_info_1, normalized_mutual_info_1 = compute_mutual_information(signal_usage[1])\n",
    "    output = [np.mean(signal_information_history[0][:10]),np.mean(signal_information_history[0][-100:]),\n",
    "                np.mean(rewards_history[0]),np.mean(rewards_history[0][-100:]),\n",
    "                np.mean(signal_information_history[1][:10]),np.mean(signal_information_history[1][-100:]),\n",
    "                np.mean(rewards_history[1]),np.mean(rewards_history[1][-100:])]\n",
    "    results+=output\n",
    "    # Adding the list as a row using loc\n",
    "    results_df.loc[len(results_df)] = results\n",
    "\n",
    "    # CASE 3\n",
    "    with_signals,full_information = False, True\n",
    "    env = NetMultiAgentEnv(n_agents=n_agents, n_features=n_features,\n",
    "                  n_signaling_actions=n_signaling_actions,\n",
    "                  n_final_actions=n_final_actions,\n",
    "                  full_information = full_information,\n",
    "                  game_dicts=randomcannonical_game,\n",
    "                  observed_variables = agents_observed_variables,\n",
    "                  agent_type=QLearningAgent,\n",
    "                  initialize = False,\n",
    "                  graph=G)\n",
    "    results = [iteration,n_signaling_actions,n_final_actions,full_information,with_signals]\n",
    "    signal_usage, rewards_history, signal_information_history, urn_histories,nature_history = simulation_function(n_agents=n_agents,\n",
    "                      n_features=n_features, n_signaling_actions=n_signaling_actions, n_final_actions=n_final_actions,\n",
    "                      n_episodes=n_episodes, with_signals = with_signals,\n",
    "                      plot=False,env=env, verbose=False)\n",
    "\n",
    "    output = [np.mean(signal_information_history[0][:10]),np.mean(signal_information_history[0][-100:]),\n",
    "                np.mean(rewards_history[0]),np.mean(rewards_history[0][-100:]),\n",
    "                np.mean(signal_information_history[1][:10]),np.mean(signal_information_history[1][-100:]),\n",
    "                np.mean(rewards_history[1]),np.mean(rewards_history[1][-100:])]\n",
    "    results+=output\n",
    "    # Adding the list as a row using loc\n",
    "    results_df.loc[len(results_df)] = results\n",
    "\n",
    "    # CASE 4\n",
    "    with_signals,full_information = True, True\n",
    "    env = NetMultiAgentEnv(n_agents=n_agents, n_features=n_features,\n",
    "                  n_signaling_actions=n_signaling_actions,\n",
    "                  n_final_actions=n_final_actions,\n",
    "                  full_information = full_information,\n",
    "                  game_dicts=randomcannonical_game,\n",
    "                  observed_variables = agents_observed_variables,\n",
    "                  agent_type=QLearningAgent,\n",
    "                  initialize = False,\n",
    "                  graph=G)\n",
    "    results = [iteration,n_signaling_actions,n_final_actions,full_information,with_signals]\n",
    "    signal_usage, rewards_history, signal_information_history, urn_histories,nature_history = simulation_function(n_agents=n_agents,\n",
    "                      n_features=n_features, n_signaling_actions=n_signaling_actions, n_final_actions=n_final_actions,\n",
    "                      n_episodes=n_episodes, with_signals = with_signals,\n",
    "                      plot=False,env=env, verbose=False)\n",
    "\n",
    "    # mutual_info_0, normalized_mutual_info_0 = compute_mutual_information(signal_usage[0])\n",
    "    # mutual_info_1, normalized_mutual_info_1 = compute_mutual_information(signal_usage[1])\n",
    "    output = [np.mean(signal_information_history[0][:10]),np.mean(signal_information_history[0][-100:]),\n",
    "                np.mean(rewards_history[0]),np.mean(rewards_history[0][-100:]),\n",
    "                np.mean(signal_information_history[1][:10]),np.mean(signal_information_history[1][-100:]),\n",
    "                np.mean(rewards_history[1]),np.mean(rewards_history[1][-100:])]\n",
    "    results+=output\n",
    "    # Adding the list as a row using loc\n",
    "    results_df.loc[len(results_df)] = results\n",
    "\n",
    "add_data = True\n",
    "if add_data:\n",
    "    old_results_df = pd.read_csv('basic_qlearning_results_cannonical_bigexplore.csv')\n",
    "    total_results_df = pd.concat([old_results_df, results_df], ignore_index=True)\n",
    "total_results_df.to_csv('basic_qlearning_results_cannonical_bigexplore.csv', index=False)\n",
    "len(total_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Complex Model\n",
    "\n",
    "- World States: Three binary variables X, Y, Z\n",
    "- agents_observed_variables = {0:[0,1],1:[1,2]}\n",
    "- n_features = 3 #parameters['n_features']\n",
    "- n_signaling_actions = 4 #parameters['n_signaling_actions']\n",
    "- n_final_actions = 8 #parameters['n_final_actions']\n",
    "- Random Games (possibly non-cannonical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define column names\n",
    "column_names = ['iteration','n_signaling_actions','n_final_actions','full_information','with_signals',\n",
    "                'Agent_0_Initial_NMI','Agent_0_NMI', 'Agent_0_avg_reward', 'Agent_0_final_reward',\n",
    "                'Agent_1_Initial_NMI','Agent_1_NMI', 'Agent_1_avg_reward','Agent_1_final_reward']\n",
    "\n",
    "# Create an empty DataFrame with the specified column names\n",
    "results_df = pd.DataFrame(columns=column_names)\n",
    "\n",
    "n_episodes = 10000\n",
    "n_iterations = 980\n",
    "\n",
    "#for iterations in tqdm(range(n_iterations), desc=\"Processing\"):\n",
    "for iteration in tqdm(range(n_iterations), desc=\"Processing processing simulations\"):\n",
    "    # get the paramteres from the dictionary\n",
    "    n_agents = 2 #parameters['n_agents']\n",
    "    n_features = 3 #parameters['n_features']\n",
    "    n_signaling_actions = 4 #parameters['n_signaling_actions']\n",
    "    n_final_actions = 8 #parameters['n_final_actions']\n",
    "\n",
    "# We get the same dictionary of games for each of the four potential setups/cases\n",
    "    # but agents have play distinct independent games at each iteration\n",
    "    random_game = {}\n",
    "    for i in range(n_agents):\n",
    "        random_game[i] = create_random_game(n_features,n_final_actions)\n",
    "    # Similarly for each of the four potential setups we keep fixed which variables are observed by each agent\n",
    "    # a dictionary of lists of the observed indexed variables\n",
    "    agents_observed_variables = {0:[0,1],1:[1,2]}\n",
    "    \n",
    "    # Graph\n",
    "    G = nx.DiGraph()\n",
    "    G.add_nodes_from([0,1])  # Adds multiple nodes at once\n",
    "    G.add_edges_from([(0, 1), (1, 0)])  # Adds multiple edges\n",
    "\n",
    "    \n",
    "    # CASE 1 , self.signal_information_history\n",
    "    with_signals,full_information = False, False\n",
    "    env = NetMultiAgentEnv(n_agents=n_agents, n_features=n_features,\n",
    "                  n_signaling_actions=n_signaling_actions,\n",
    "                  n_final_actions=n_final_actions,\n",
    "                  full_information = full_information,\n",
    "                  game_dicts=random_game,\n",
    "                  observed_variables = agents_observed_variables,\n",
    "                  agent_type=QLearningAgent,\n",
    "                  initialize = False,\n",
    "                  graph=G)\n",
    "    results = [iteration,n_signaling_actions,n_final_actions,full_information,with_signals]\n",
    "    signal_usage, rewards_history, signal_information_history, urn_histories,nature_history = simulation_function(n_agents=n_agents,\n",
    "                      n_features=n_features, n_signaling_actions=n_signaling_actions, n_final_actions=n_final_actions,\n",
    "                      n_episodes=n_episodes, with_signals = with_signals,\n",
    "                      plot=False,env=env, verbose=False)\n",
    " \n",
    "    output = [np.mean(signal_information_history[0][:10]),np.mean(signal_information_history[0][-100:]),\n",
    "              np.mean(rewards_history[0]),np.mean(rewards_history[0][-100:]),\n",
    "              np.mean(signal_information_history[1][:10]),np.mean(signal_information_history[1][-100:]),\n",
    "              np.mean(rewards_history[1]),np.mean(rewards_history[1][-100:])]\n",
    "    results+=output\n",
    "    # Adding the list as a row using loc\n",
    "    results_df.loc[len(results_df)] = results\n",
    "\n",
    "    # CASE 2\n",
    "    with_signals,full_information = True, False\n",
    "    env = NetMultiAgentEnv(n_agents=n_agents, n_features=n_features,\n",
    "                  n_signaling_actions=n_signaling_actions,\n",
    "                  n_final_actions=n_final_actions,\n",
    "                  full_information = full_information,\n",
    "                  game_dicts=random_game,\n",
    "                  observed_variables = agents_observed_variables,\n",
    "                  agent_type=QLearningAgent,\n",
    "                  initialize = False,\n",
    "                  graph=G)\n",
    "    results = [iteration,n_signaling_actions,n_final_actions,full_information,with_signals]\n",
    "    signal_usage, rewards_history, signal_information_history, urn_histories,nature_history = simulation_function(n_agents=n_agents,\n",
    "                      n_features=n_features, n_signaling_actions=n_signaling_actions, n_final_actions=n_final_actions,\n",
    "                      n_episodes=n_episodes, with_signals = with_signals,\n",
    "                      plot=False,env=env, verbose=False)\n",
    "\n",
    "    # mutual_info_0, normalized_mutual_info_0 = compute_mutual_information(signal_usage[0])\n",
    "    # mutual_info_1, normalized_mutual_info_1 = compute_mutual_information(signal_usage[1])\n",
    "    output = [np.mean(signal_information_history[0][:10]),np.mean(signal_information_history[0][-100:]),\n",
    "              np.mean(rewards_history[0]),np.mean(rewards_history[0][-100:]),\n",
    "              np.mean(signal_information_history[1][:10]),np.mean(signal_information_history[1][-100:]),\n",
    "              np.mean(rewards_history[1]),np.mean(rewards_history[1][-100:])]\n",
    "    results+=output\n",
    "    # Adding the list as a row using loc\n",
    "    results_df.loc[len(results_df)] = results\n",
    "\n",
    "    # CASE 3\n",
    "    with_signals,full_information = False, True\n",
    "    env = NetMultiAgentEnv(n_agents=n_agents, n_features=n_features,\n",
    "                  n_signaling_actions=n_signaling_actions,\n",
    "                  n_final_actions=n_final_actions,\n",
    "                  full_information = full_information,\n",
    "                  game_dicts=random_game,\n",
    "                  observed_variables = agents_observed_variables,\n",
    "                  agent_type=QLearningAgent,\n",
    "                  initialize = False,\n",
    "                  graph=G)\n",
    "    results = [iteration,n_signaling_actions,n_final_actions,full_information,with_signals]\n",
    "    signal_usage, rewards_history, signal_information_history, urn_histories,nature_history = simulation_function(n_agents=n_agents,\n",
    "                      n_features=n_features, n_signaling_actions=n_signaling_actions, n_final_actions=n_final_actions,\n",
    "                      n_episodes=n_episodes, with_signals = with_signals,\n",
    "                      plot=False,env=env, verbose=False)\n",
    "\n",
    "    # mutual_info_0, normalized_mutual_info_0 = compute_mutual_information(signal_usage[0])\n",
    "    # mutual_info_1, normalized_mutual_info_1 = compute_mutual_information(signal_usage[1])\n",
    "    output = [np.mean(signal_information_history[0][:10]),np.mean(signal_information_history[0][-100:]),\n",
    "              np.mean(rewards_history[0]),np.mean(rewards_history[0][-100:]),\n",
    "              np.mean(signal_information_history[1][:10]),np.mean(signal_information_history[1][-100:]),\n",
    "              np.mean(rewards_history[1]),np.mean(rewards_history[1][-100:])]\n",
    "    results+=output\n",
    "    # Adding the list as a row using loc\n",
    "    results_df.loc[len(results_df)] = results\n",
    "\n",
    "    # CASE 4\n",
    "    with_signals,full_information = True, True\n",
    "    env = NetMultiAgentEnv(n_agents=n_agents, n_features=n_features,\n",
    "                  n_signaling_actions=n_signaling_actions,\n",
    "                  n_final_actions=n_final_actions,\n",
    "                  full_information = full_information,\n",
    "                  game_dicts=random_game,\n",
    "                  observed_variables = agents_observed_variables,\n",
    "                  agent_type=QLearningAgent,\n",
    "                  initialize = False,\n",
    "                  graph=G)\n",
    "    results = [iteration,n_signaling_actions,n_final_actions,full_information,with_signals]\n",
    "    signal_usage, rewards_history, signal_information_history, urn_histories,nature_history = simulation_function(n_agents=n_agents,\n",
    "                      n_features=n_features, n_signaling_actions=n_signaling_actions, n_final_actions=n_final_actions,\n",
    "                      n_episodes=n_episodes, with_signals = with_signals,\n",
    "                      plot=False,env=env, verbose=False)\n",
    "\n",
    "    # mutual_info_0, normalized_mutual_info_0 = compute_mutual_information(signal_usage[0])\n",
    "    # mutual_info_1, normalized_mutual_info_1 = compute_mutual_information(signal_usage[1])\n",
    "    output = [np.mean(signal_information_history[0][:10]),np.mean(signal_information_history[0][-100:]),\n",
    "              np.mean(rewards_history[0]),np.mean(rewards_history[0][-100:]),\n",
    "              np.mean(signal_information_history[1][:10]),np.mean(signal_information_history[1][-100:]),\n",
    "              np.mean(rewards_history[1]),np.mean(rewards_history[1][-100:])]\n",
    "    results+=output\n",
    "    # Adding the list as a row using loc\n",
    "    results_df.loc[len(results_df)] = results\n",
    "\n",
    "add_data = True\n",
    "if add_data:\n",
    "    old_results_df = pd.read_csv('qlearning_results_bigexplore.csv')\n",
    "    total_results_df = pd.concat([old_results_df, results_df], ignore_index=True)\n",
    "total_results_df.to_csv('qlearning_results_bigexplore.csv', index=False)\n",
    "len(total_results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
